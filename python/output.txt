


def forward(self, L_input_ids_ : torch.Tensor, L_self_modules_embeddings_buffers_token_type_ids_ : torch.Tensor, L_self_modules_embeddings_buffers_position_ids_ : torch.Tensor, L_self_modules_embeddings_modules_word_embeddings_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_position_embeddings_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_attention_mask_ : torch.Tensor, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_pooler_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_pooler_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter):
    l_input_ids_ = L_input_ids_
    l_self_modules_embeddings_buffers_token_type_ids_ = L_self_modules_embeddings_buffers_token_type_ids_
    l_self_modules_embeddings_buffers_position_ids_ = L_self_modules_embeddings_buffers_position_ids_
    l_self_modules_embeddings_modules_word_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_word_embeddings_parameters_weight_
    l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_
    l_self_modules_embeddings_modules_position_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_position_embeddings_parameters_weight_
    l_self_modules_embeddings_modules_layer_norm_parameters_weight_ = L_self_modules_embeddings_modules_LayerNorm_parameters_weight_
    l_self_modules_embeddings_modules_layer_norm_parameters_bias_ = L_self_modules_embeddings_modules_LayerNorm_parameters_bias_
    l_attention_mask_ = L_attention_mask_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_pooler_modules_dense_parameters_weight_ = L_self_modules_pooler_modules_dense_parameters_weight_
    l_self_modules_pooler_modules_dense_parameters_bias_ = L_self_modules_pooler_modules_dense_parameters_bias_
    buffered_token_type_ids = l_self_modules_embeddings_buffers_token_type_ids_[(slice(None, None, None), slice(None, 128, None))];  l_self_modules_embeddings_buffers_token_type_ids_ = None
    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(1, 128);  buffered_token_type_ids = None
    position_ids = l_self_modules_embeddings_buffers_position_ids_[(slice(None, None, None), slice(0, 128, None))];  l_self_modules_embeddings_buffers_position_ids_ = None
    inputs_embeds = torch.nn.functional.embedding(l_input_ids_, l_self_modules_embeddings_modules_word_embeddings_parameters_weight_, 0, None, 2.0, False, False);  l_input_ids_ = l_self_modules_embeddings_modules_word_embeddings_parameters_weight_ = None
    token_type_embeddings = torch.nn.functional.embedding(buffered_token_type_ids_expanded, l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_, None, None, 2.0, False, False);  buffered_token_type_ids_expanded = l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ = None
    embeddings = inputs_embeds + token_type_embeddings;  inputs_embeds = token_type_embeddings = None
    position_embeddings = torch.nn.functional.embedding(position_ids, l_self_modules_embeddings_modules_position_embeddings_parameters_weight_, None, None, 2.0, False, False);  position_ids = l_self_modules_embeddings_modules_position_embeddings_parameters_weight_ = None
    embeddings += position_embeddings;  embeddings_1 = embeddings;  embeddings = position_embeddings = None
    embeddings_2 = torch.nn.functional.layer_norm(embeddings_1, (768,), l_self_modules_embeddings_modules_layer_norm_parameters_weight_, l_self_modules_embeddings_modules_layer_norm_parameters_bias_, 1e-12);  embeddings_1 = l_self_modules_embeddings_modules_layer_norm_parameters_weight_ = l_self_modules_embeddings_modules_layer_norm_parameters_bias_ = None
    embeddings_3 = torch.nn.functional.dropout(embeddings_2, 0.1, False, False);  embeddings_2 = None
    getitem_2 = l_attention_mask_[(slice(None, None, None), None, None, slice(None, None, None))];  l_attention_mask_ = None
    expand_1 = getitem_2.expand(1, 1, 128, 128);  getitem_2 = None
    expanded_mask = expand_1.to(torch.float32);  expand_1 = None
    inverted_mask = 1.0 - expanded_mask;  expanded_mask = None
    to_1 = inverted_mask.to(torch.bool)
    extended_attention_mask = inverted_mask.masked_fill(to_1, -3.4028234663852886e+38);  inverted_mask = to_1 = None
    linear = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x = linear.view((1, 128, 12, 64));  linear = None
    query_layer = x.permute(0, 2, 1, 3);  x = None
    linear_1 = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_1 = linear_1.view((1, 128, 12, 64));  linear_1 = None
    key_layer = x_1.permute(0, 2, 1, 3);  x_1 = None
    linear_2 = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_2 = linear_2.view((1, 128, 12, 64));  linear_2 = None
    value_layer = x_2.permute(0, 2, 1, 3);  x_2 = None
    attn_output = torch._C._nn.scaled_dot_product_attention(query_layer, key_layer, value_layer, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer = key_layer = value_layer = None
    attn_output_1 = attn_output.transpose(1, 2);  attn_output = None
    attn_output_2 = attn_output_1.reshape(1, 128, 768);  attn_output_1 = None
    hidden_states = torch._C._nn.linear(attn_output_2, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_2 = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_1 = torch.nn.functional.dropout(hidden_states, 0.1, False, False);  hidden_states = None
    add_1 = hidden_states_1 + embeddings_3;  hidden_states_1 = embeddings_3 = None
    hidden_states_2 = torch.nn.functional.layer_norm(add_1, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_1 = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_3 = torch._C._nn.linear(hidden_states_2, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_4 = torch._C._nn.gelu(hidden_states_3);  hidden_states_3 = None
    hidden_states_5 = torch._C._nn.linear(hidden_states_4, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_);  hidden_states_4 = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_6 = torch.nn.functional.dropout(hidden_states_5, 0.1, False, False);  hidden_states_5 = None
    add_2 = hidden_states_6 + hidden_states_2;  hidden_states_6 = hidden_states_2 = None
    hidden_states_7 = torch.nn.functional.layer_norm(add_2, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_2 = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_6 = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_3 = linear_6.view((1, 128, 12, 64));  linear_6 = None
    query_layer_1 = x_3.permute(0, 2, 1, 3);  x_3 = None
    linear_7 = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_4 = linear_7.view((1, 128, 12, 64));  linear_7 = None
    key_layer_1 = x_4.permute(0, 2, 1, 3);  x_4 = None
    linear_8 = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_5 = linear_8.view((1, 128, 12, 64));  linear_8 = None
    value_layer_1 = x_5.permute(0, 2, 1, 3);  x_5 = None
    attn_output_3 = torch._C._nn.scaled_dot_product_attention(query_layer_1, key_layer_1, value_layer_1, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_1 = key_layer_1 = value_layer_1 = None
    attn_output_4 = attn_output_3.transpose(1, 2);  attn_output_3 = None
    attn_output_5 = attn_output_4.reshape(1, 128, 768);  attn_output_4 = None
    hidden_states_8 = torch._C._nn.linear(attn_output_5, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_5 = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_9 = torch.nn.functional.dropout(hidden_states_8, 0.1, False, False);  hidden_states_8 = None
    add_3 = hidden_states_9 + hidden_states_7;  hidden_states_9 = hidden_states_7 = None
    hidden_states_10 = torch.nn.functional.layer_norm(add_3, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_3 = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_11 = torch._C._nn.linear(hidden_states_10, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_12 = torch._C._nn.gelu(hidden_states_11);  hidden_states_11 = None
    hidden_states_13 = torch._C._nn.linear(hidden_states_12, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_);  hidden_states_12 = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_14 = torch.nn.functional.dropout(hidden_states_13, 0.1, False, False);  hidden_states_13 = None
    add_4 = hidden_states_14 + hidden_states_10;  hidden_states_14 = hidden_states_10 = None
    hidden_states_15 = torch.nn.functional.layer_norm(add_4, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_4 = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_12 = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_6 = linear_12.view((1, 128, 12, 64));  linear_12 = None
    query_layer_2 = x_6.permute(0, 2, 1, 3);  x_6 = None
    linear_13 = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_7 = linear_13.view((1, 128, 12, 64));  linear_13 = None
    key_layer_2 = x_7.permute(0, 2, 1, 3);  x_7 = None
    linear_14 = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_8 = linear_14.view((1, 128, 12, 64));  linear_14 = None
    value_layer_2 = x_8.permute(0, 2, 1, 3);  x_8 = None
    attn_output_6 = torch._C._nn.scaled_dot_product_attention(query_layer_2, key_layer_2, value_layer_2, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_2 = key_layer_2 = value_layer_2 = None
    attn_output_7 = attn_output_6.transpose(1, 2);  attn_output_6 = None
    attn_output_8 = attn_output_7.reshape(1, 128, 768);  attn_output_7 = None
    hidden_states_16 = torch._C._nn.linear(attn_output_8, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_8 = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_17 = torch.nn.functional.dropout(hidden_states_16, 0.1, False, False);  hidden_states_16 = None
    add_5 = hidden_states_17 + hidden_states_15;  hidden_states_17 = hidden_states_15 = None
    hidden_states_18 = torch.nn.functional.layer_norm(add_5, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_5 = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_19 = torch._C._nn.linear(hidden_states_18, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_20 = torch._C._nn.gelu(hidden_states_19);  hidden_states_19 = None
    hidden_states_21 = torch._C._nn.linear(hidden_states_20, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_);  hidden_states_20 = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_22 = torch.nn.functional.dropout(hidden_states_21, 0.1, False, False);  hidden_states_21 = None
    add_6 = hidden_states_22 + hidden_states_18;  hidden_states_22 = hidden_states_18 = None
    hidden_states_23 = torch.nn.functional.layer_norm(add_6, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_6 = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_18 = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_9 = linear_18.view((1, 128, 12, 64));  linear_18 = None
    query_layer_3 = x_9.permute(0, 2, 1, 3);  x_9 = None
    linear_19 = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_10 = linear_19.view((1, 128, 12, 64));  linear_19 = None
    key_layer_3 = x_10.permute(0, 2, 1, 3);  x_10 = None
    linear_20 = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_11 = linear_20.view((1, 128, 12, 64));  linear_20 = None
    value_layer_3 = x_11.permute(0, 2, 1, 3);  x_11 = None
    attn_output_9 = torch._C._nn.scaled_dot_product_attention(query_layer_3, key_layer_3, value_layer_3, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_3 = key_layer_3 = value_layer_3 = None
    attn_output_10 = attn_output_9.transpose(1, 2);  attn_output_9 = None
    attn_output_11 = attn_output_10.reshape(1, 128, 768);  attn_output_10 = None
    hidden_states_24 = torch._C._nn.linear(attn_output_11, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_11 = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_25 = torch.nn.functional.dropout(hidden_states_24, 0.1, False, False);  hidden_states_24 = None
    add_7 = hidden_states_25 + hidden_states_23;  hidden_states_25 = hidden_states_23 = None
    hidden_states_26 = torch.nn.functional.layer_norm(add_7, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_7 = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_27 = torch._C._nn.linear(hidden_states_26, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_28 = torch._C._nn.gelu(hidden_states_27);  hidden_states_27 = None
    hidden_states_29 = torch._C._nn.linear(hidden_states_28, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_);  hidden_states_28 = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_30 = torch.nn.functional.dropout(hidden_states_29, 0.1, False, False);  hidden_states_29 = None
    add_8 = hidden_states_30 + hidden_states_26;  hidden_states_30 = hidden_states_26 = None
    hidden_states_31 = torch.nn.functional.layer_norm(add_8, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_8 = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_24 = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_12 = linear_24.view((1, 128, 12, 64));  linear_24 = None
    query_layer_4 = x_12.permute(0, 2, 1, 3);  x_12 = None
    linear_25 = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_13 = linear_25.view((1, 128, 12, 64));  linear_25 = None
    key_layer_4 = x_13.permute(0, 2, 1, 3);  x_13 = None
    linear_26 = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_14 = linear_26.view((1, 128, 12, 64));  linear_26 = None
    value_layer_4 = x_14.permute(0, 2, 1, 3);  x_14 = None
    attn_output_12 = torch._C._nn.scaled_dot_product_attention(query_layer_4, key_layer_4, value_layer_4, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_4 = key_layer_4 = value_layer_4 = None
    attn_output_13 = attn_output_12.transpose(1, 2);  attn_output_12 = None
    attn_output_14 = attn_output_13.reshape(1, 128, 768);  attn_output_13 = None
    hidden_states_32 = torch._C._nn.linear(attn_output_14, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_14 = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_33 = torch.nn.functional.dropout(hidden_states_32, 0.1, False, False);  hidden_states_32 = None
    add_9 = hidden_states_33 + hidden_states_31;  hidden_states_33 = hidden_states_31 = None
    hidden_states_34 = torch.nn.functional.layer_norm(add_9, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_9 = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_35 = torch._C._nn.linear(hidden_states_34, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_36 = torch._C._nn.gelu(hidden_states_35);  hidden_states_35 = None
    hidden_states_37 = torch._C._nn.linear(hidden_states_36, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_);  hidden_states_36 = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_38 = torch.nn.functional.dropout(hidden_states_37, 0.1, False, False);  hidden_states_37 = None
    add_10 = hidden_states_38 + hidden_states_34;  hidden_states_38 = hidden_states_34 = None
    hidden_states_39 = torch.nn.functional.layer_norm(add_10, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_10 = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_30 = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_15 = linear_30.view((1, 128, 12, 64));  linear_30 = None
    query_layer_5 = x_15.permute(0, 2, 1, 3);  x_15 = None
    linear_31 = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_16 = linear_31.view((1, 128, 12, 64));  linear_31 = None
    key_layer_5 = x_16.permute(0, 2, 1, 3);  x_16 = None
    linear_32 = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_17 = linear_32.view((1, 128, 12, 64));  linear_32 = None
    value_layer_5 = x_17.permute(0, 2, 1, 3);  x_17 = None
    attn_output_15 = torch._C._nn.scaled_dot_product_attention(query_layer_5, key_layer_5, value_layer_5, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_5 = key_layer_5 = value_layer_5 = None
    attn_output_16 = attn_output_15.transpose(1, 2);  attn_output_15 = None
    attn_output_17 = attn_output_16.reshape(1, 128, 768);  attn_output_16 = None
    hidden_states_40 = torch._C._nn.linear(attn_output_17, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_17 = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_41 = torch.nn.functional.dropout(hidden_states_40, 0.1, False, False);  hidden_states_40 = None
    add_11 = hidden_states_41 + hidden_states_39;  hidden_states_41 = hidden_states_39 = None
    hidden_states_42 = torch.nn.functional.layer_norm(add_11, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_11 = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_43 = torch._C._nn.linear(hidden_states_42, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_44 = torch._C._nn.gelu(hidden_states_43);  hidden_states_43 = None
    hidden_states_45 = torch._C._nn.linear(hidden_states_44, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_);  hidden_states_44 = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_46 = torch.nn.functional.dropout(hidden_states_45, 0.1, False, False);  hidden_states_45 = None
    add_12 = hidden_states_46 + hidden_states_42;  hidden_states_46 = hidden_states_42 = None
    hidden_states_47 = torch.nn.functional.layer_norm(add_12, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_12 = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_36 = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_18 = linear_36.view((1, 128, 12, 64));  linear_36 = None
    query_layer_6 = x_18.permute(0, 2, 1, 3);  x_18 = None
    linear_37 = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_19 = linear_37.view((1, 128, 12, 64));  linear_37 = None
    key_layer_6 = x_19.permute(0, 2, 1, 3);  x_19 = None
    linear_38 = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_20 = linear_38.view((1, 128, 12, 64));  linear_38 = None
    value_layer_6 = x_20.permute(0, 2, 1, 3);  x_20 = None
    attn_output_18 = torch._C._nn.scaled_dot_product_attention(query_layer_6, key_layer_6, value_layer_6, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_6 = key_layer_6 = value_layer_6 = None
    attn_output_19 = attn_output_18.transpose(1, 2);  attn_output_18 = None
    attn_output_20 = attn_output_19.reshape(1, 128, 768);  attn_output_19 = None
    hidden_states_48 = torch._C._nn.linear(attn_output_20, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_20 = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_49 = torch.nn.functional.dropout(hidden_states_48, 0.1, False, False);  hidden_states_48 = None
    add_13 = hidden_states_49 + hidden_states_47;  hidden_states_49 = hidden_states_47 = None
    hidden_states_50 = torch.nn.functional.layer_norm(add_13, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_13 = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_51 = torch._C._nn.linear(hidden_states_50, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_52 = torch._C._nn.gelu(hidden_states_51);  hidden_states_51 = None
    hidden_states_53 = torch._C._nn.linear(hidden_states_52, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_);  hidden_states_52 = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_54 = torch.nn.functional.dropout(hidden_states_53, 0.1, False, False);  hidden_states_53 = None
    add_14 = hidden_states_54 + hidden_states_50;  hidden_states_54 = hidden_states_50 = None
    hidden_states_55 = torch.nn.functional.layer_norm(add_14, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_14 = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_42 = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_21 = linear_42.view((1, 128, 12, 64));  linear_42 = None
    query_layer_7 = x_21.permute(0, 2, 1, 3);  x_21 = None
    linear_43 = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_22 = linear_43.view((1, 128, 12, 64));  linear_43 = None
    key_layer_7 = x_22.permute(0, 2, 1, 3);  x_22 = None
    linear_44 = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_23 = linear_44.view((1, 128, 12, 64));  linear_44 = None
    value_layer_7 = x_23.permute(0, 2, 1, 3);  x_23 = None
    attn_output_21 = torch._C._nn.scaled_dot_product_attention(query_layer_7, key_layer_7, value_layer_7, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_7 = key_layer_7 = value_layer_7 = None
    attn_output_22 = attn_output_21.transpose(1, 2);  attn_output_21 = None
    attn_output_23 = attn_output_22.reshape(1, 128, 768);  attn_output_22 = None
    hidden_states_56 = torch._C._nn.linear(attn_output_23, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_23 = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_57 = torch.nn.functional.dropout(hidden_states_56, 0.1, False, False);  hidden_states_56 = None
    add_15 = hidden_states_57 + hidden_states_55;  hidden_states_57 = hidden_states_55 = None
    hidden_states_58 = torch.nn.functional.layer_norm(add_15, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_15 = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_59 = torch._C._nn.linear(hidden_states_58, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_60 = torch._C._nn.gelu(hidden_states_59);  hidden_states_59 = None
    hidden_states_61 = torch._C._nn.linear(hidden_states_60, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_);  hidden_states_60 = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_62 = torch.nn.functional.dropout(hidden_states_61, 0.1, False, False);  hidden_states_61 = None
    add_16 = hidden_states_62 + hidden_states_58;  hidden_states_62 = hidden_states_58 = None
    hidden_states_63 = torch.nn.functional.layer_norm(add_16, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_16 = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_48 = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_24 = linear_48.view((1, 128, 12, 64));  linear_48 = None
    query_layer_8 = x_24.permute(0, 2, 1, 3);  x_24 = None
    linear_49 = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_25 = linear_49.view((1, 128, 12, 64));  linear_49 = None
    key_layer_8 = x_25.permute(0, 2, 1, 3);  x_25 = None
    linear_50 = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_26 = linear_50.view((1, 128, 12, 64));  linear_50 = None
    value_layer_8 = x_26.permute(0, 2, 1, 3);  x_26 = None
    attn_output_24 = torch._C._nn.scaled_dot_product_attention(query_layer_8, key_layer_8, value_layer_8, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_8 = key_layer_8 = value_layer_8 = None
    attn_output_25 = attn_output_24.transpose(1, 2);  attn_output_24 = None
    attn_output_26 = attn_output_25.reshape(1, 128, 768);  attn_output_25 = None
    hidden_states_64 = torch._C._nn.linear(attn_output_26, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_26 = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_65 = torch.nn.functional.dropout(hidden_states_64, 0.1, False, False);  hidden_states_64 = None
    add_17 = hidden_states_65 + hidden_states_63;  hidden_states_65 = hidden_states_63 = None
    hidden_states_66 = torch.nn.functional.layer_norm(add_17, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_17 = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_67 = torch._C._nn.linear(hidden_states_66, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_68 = torch._C._nn.gelu(hidden_states_67);  hidden_states_67 = None
    hidden_states_69 = torch._C._nn.linear(hidden_states_68, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_);  hidden_states_68 = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_70 = torch.nn.functional.dropout(hidden_states_69, 0.1, False, False);  hidden_states_69 = None
    add_18 = hidden_states_70 + hidden_states_66;  hidden_states_70 = hidden_states_66 = None
    hidden_states_71 = torch.nn.functional.layer_norm(add_18, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_18 = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_54 = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_27 = linear_54.view((1, 128, 12, 64));  linear_54 = None
    query_layer_9 = x_27.permute(0, 2, 1, 3);  x_27 = None
    linear_55 = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_28 = linear_55.view((1, 128, 12, 64));  linear_55 = None
    key_layer_9 = x_28.permute(0, 2, 1, 3);  x_28 = None
    linear_56 = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_29 = linear_56.view((1, 128, 12, 64));  linear_56 = None
    value_layer_9 = x_29.permute(0, 2, 1, 3);  x_29 = None
    attn_output_27 = torch._C._nn.scaled_dot_product_attention(query_layer_9, key_layer_9, value_layer_9, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_9 = key_layer_9 = value_layer_9 = None
    attn_output_28 = attn_output_27.transpose(1, 2);  attn_output_27 = None
    attn_output_29 = attn_output_28.reshape(1, 128, 768);  attn_output_28 = None
    hidden_states_72 = torch._C._nn.linear(attn_output_29, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_29 = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_73 = torch.nn.functional.dropout(hidden_states_72, 0.1, False, False);  hidden_states_72 = None
    add_19 = hidden_states_73 + hidden_states_71;  hidden_states_73 = hidden_states_71 = None
    hidden_states_74 = torch.nn.functional.layer_norm(add_19, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_19 = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_75 = torch._C._nn.linear(hidden_states_74, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_76 = torch._C._nn.gelu(hidden_states_75);  hidden_states_75 = None
    hidden_states_77 = torch._C._nn.linear(hidden_states_76, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_);  hidden_states_76 = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_78 = torch.nn.functional.dropout(hidden_states_77, 0.1, False, False);  hidden_states_77 = None
    add_20 = hidden_states_78 + hidden_states_74;  hidden_states_78 = hidden_states_74 = None
    hidden_states_79 = torch.nn.functional.layer_norm(add_20, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_20 = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_60 = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_30 = linear_60.view((1, 128, 12, 64));  linear_60 = None
    query_layer_10 = x_30.permute(0, 2, 1, 3);  x_30 = None
    linear_61 = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_31 = linear_61.view((1, 128, 12, 64));  linear_61 = None
    key_layer_10 = x_31.permute(0, 2, 1, 3);  x_31 = None
    linear_62 = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_32 = linear_62.view((1, 128, 12, 64));  linear_62 = None
    value_layer_10 = x_32.permute(0, 2, 1, 3);  x_32 = None
    attn_output_30 = torch._C._nn.scaled_dot_product_attention(query_layer_10, key_layer_10, value_layer_10, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_10 = key_layer_10 = value_layer_10 = None
    attn_output_31 = attn_output_30.transpose(1, 2);  attn_output_30 = None
    attn_output_32 = attn_output_31.reshape(1, 128, 768);  attn_output_31 = None
    hidden_states_80 = torch._C._nn.linear(attn_output_32, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_32 = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_81 = torch.nn.functional.dropout(hidden_states_80, 0.1, False, False);  hidden_states_80 = None
    add_21 = hidden_states_81 + hidden_states_79;  hidden_states_81 = hidden_states_79 = None
    hidden_states_82 = torch.nn.functional.layer_norm(add_21, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_21 = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_83 = torch._C._nn.linear(hidden_states_82, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_84 = torch._C._nn.gelu(hidden_states_83);  hidden_states_83 = None
    hidden_states_85 = torch._C._nn.linear(hidden_states_84, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_);  hidden_states_84 = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_86 = torch.nn.functional.dropout(hidden_states_85, 0.1, False, False);  hidden_states_85 = None
    add_22 = hidden_states_86 + hidden_states_82;  hidden_states_86 = hidden_states_82 = None
    hidden_states_87 = torch.nn.functional.layer_norm(add_22, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_22 = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_66 = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_33 = linear_66.view((1, 128, 12, 64));  linear_66 = None
    query_layer_11 = x_33.permute(0, 2, 1, 3);  x_33 = None
    linear_67 = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_34 = linear_67.view((1, 128, 12, 64));  linear_67 = None
    key_layer_11 = x_34.permute(0, 2, 1, 3);  x_34 = None
    linear_68 = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_35 = linear_68.view((1, 128, 12, 64));  linear_68 = None
    value_layer_11 = x_35.permute(0, 2, 1, 3);  x_35 = None
    attn_output_33 = torch._C._nn.scaled_dot_product_attention(query_layer_11, key_layer_11, value_layer_11, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_11 = key_layer_11 = value_layer_11 = extended_attention_mask = None
    attn_output_34 = attn_output_33.transpose(1, 2);  attn_output_33 = None
    attn_output_35 = attn_output_34.reshape(1, 128, 768);  attn_output_34 = None
    hidden_states_88 = torch._C._nn.linear(attn_output_35, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_35 = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_89 = torch.nn.functional.dropout(hidden_states_88, 0.1, False, False);  hidden_states_88 = None
    add_23 = hidden_states_89 + hidden_states_87;  hidden_states_89 = hidden_states_87 = None
    hidden_states_90 = torch.nn.functional.layer_norm(add_23, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_23 = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_91 = torch._C._nn.linear(hidden_states_90, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_92 = torch._C._nn.gelu(hidden_states_91);  hidden_states_91 = None
    hidden_states_93 = torch._C._nn.linear(hidden_states_92, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_);  hidden_states_92 = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_94 = torch.nn.functional.dropout(hidden_states_93, 0.1, False, False);  hidden_states_93 = None
    add_24 = hidden_states_94 + hidden_states_90;  hidden_states_94 = hidden_states_90 = None
    hidden_states_95 = torch.nn.functional.layer_norm(add_24, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_24 = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_ = None
    first_token_tensor = hidden_states_95[(slice(None, None, None), 0)]
    pooled_output = torch._C._nn.linear(first_token_tensor, l_self_modules_pooler_modules_dense_parameters_weight_, l_self_modules_pooler_modules_dense_parameters_bias_);  first_token_tensor = l_self_modules_pooler_modules_dense_parameters_weight_ = l_self_modules_pooler_modules_dense_parameters_bias_ = None
    pooled_output_1 = torch.tanh(pooled_output);  pooled_output = None
    return (hidden_states_95, pooled_output_1)
    
custom backend called with FX graph:
opcode         name                                                                                                                    target                                                                                                                 args                                                                                                                                                                                                                                                                   kwargs
-------------  ----------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------
placeholder    l_input_ids_                                                                                                            L_input_ids_                                                                                                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_embeddings_buffers_token_type_ids_                                                                       L_self_modules_embeddings_buffers_token_type_ids_                                                                      ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_embeddings_buffers_position_ids_                                                                         L_self_modules_embeddings_buffers_position_ids_                                                                        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_embeddings_modules_word_embeddings_parameters_weight_                                                    L_self_modules_embeddings_modules_word_embeddings_parameters_weight_                                                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_                                              L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_                                             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_embeddings_modules_position_embeddings_parameters_weight_                                                L_self_modules_embeddings_modules_position_embeddings_parameters_weight_                                               ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_embeddings_modules_layer_norm_parameters_weight_                                                         L_self_modules_embeddings_modules_LayerNorm_parameters_weight_                                                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_embeddings_modules_layer_norm_parameters_bias_                                                           L_self_modules_embeddings_modules_LayerNorm_parameters_bias_                                                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_attention_mask_                                                                                                       L_attention_mask_                                                                                                      ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_         L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_           L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_           L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_             L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_            ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_         L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_           L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_       L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_      ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_         L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_  L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_  ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_    L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_    ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_                   L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_                  ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_                     L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_                    ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_                         L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_                        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_                           L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_                          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_                    ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_                      ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_         L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_           L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_           L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_             L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_            ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_         L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_           L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_       L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_      ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_         L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_  L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_  ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_    L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_    ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_                   L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_                  ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_                     L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_                    ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_                         L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_                        ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_                           L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_                          ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_                    ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_                      ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_pooler_modules_dense_parameters_weight_                                                                  L_self_modules_pooler_modules_dense_parameters_weight_                                                                 ()                                                                                                                                                                                                                                                                     {}
placeholder    l_self_modules_pooler_modules_dense_parameters_bias_                                                                    L_self_modules_pooler_modules_dense_parameters_bias_                                                                   ()                                                                                                                                                                                                                                                                     {}
call_function  buffered_token_type_ids                                                                                                 <built-in function getitem>                                                                                            (l_self_modules_embeddings_buffers_token_type_ids_, (slice(None, None, None), slice(None, 128, None)))                                                                                                                                                                 {}
call_method    buffered_token_type_ids_expanded                                                                                        expand                                                                                                                 (buffered_token_type_ids, 1, 128)                                                                                                                                                                                                                                      {}
call_function  position_ids                                                                                                            <built-in function getitem>                                                                                            (l_self_modules_embeddings_buffers_position_ids_, (slice(None, None, None), slice(0, 128, None)))                                                                                                                                                                      {}
call_function  inputs_embeds                                                                                                           <function embedding at 0x1139af380>                                                                                    (l_input_ids_, l_self_modules_embeddings_modules_word_embeddings_parameters_weight_, 0, None, 2.0, False, False)                                                                                                                                                       {}
call_function  token_type_embeddings                                                                                                   <function embedding at 0x1139af380>                                                                                    (buffered_token_type_ids_expanded, l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_, None, None, 2.0, False, False)                                                                                                                          {}
call_function  embeddings                                                                                                              <built-in function add>                                                                                                (inputs_embeds, token_type_embeddings)                                                                                                                                                                                                                                 {}
call_function  position_embeddings                                                                                                     <function embedding at 0x1139af380>                                                                                    (position_ids, l_self_modules_embeddings_modules_position_embeddings_parameters_weight_, None, None, 2.0, False, False)                                                                                                                                                {}
call_function  embeddings_1                                                                                                            <built-in function iadd>                                                                                               (embeddings, position_embeddings)                                                                                                                                                                                                                                      {}
call_function  embeddings_2                                                                                                            <function layer_norm at 0x1139af740>                                                                                   (embeddings_1, (768,), l_self_modules_embeddings_modules_layer_norm_parameters_weight_, l_self_modules_embeddings_modules_layer_norm_parameters_bias_, 1e-12)                                                                                                          {}
call_function  embeddings_3                                                                                                            <function dropout at 0x1139ae0c0>                                                                                      (embeddings_2, 0.1, False, False)                                                                                                                                                                                                                                      {}
call_function  getitem_2                                                                                                               <built-in function getitem>                                                                                            (l_attention_mask_, (slice(None, None, None), None, None, slice(None, None, None)))                                                                                                                                                                                    {}
call_method    expand_1                                                                                                                expand                                                                                                                 (getitem_2, 1, 1, 128, 128)                                                                                                                                                                                                                                            {}
call_method    expanded_mask                                                                                                           to                                                                                                                     (expand_1, torch.float32)                                                                                                                                                                                                                                              {}
call_function  inverted_mask                                                                                                           <built-in function sub>                                                                                                (1.0, expanded_mask)                                                                                                                                                                                                                                                   {}
call_method    to_1                                                                                                                    to                                                                                                                     (inverted_mask, torch.bool)                                                                                                                                                                                                                                            {}
call_method    extended_attention_mask                                                                                                 masked_fill                                                                                                            (inverted_mask, to_1, -3.4028234663852886e+38)                                                                                                                                                                                                                         {}
call_function  linear                                                                                                                  <built-in function linear>                                                                                             (embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_)                           {}
call_method    x                                                                                                                       view                                                                                                                   (linear, (1, 128, 12, 64))                                                                                                                                                                                                                                             {}
call_method    query_layer                                                                                                             permute                                                                                                                (x, 0, 2, 1, 3)                                                                                                                                                                                                                                                        {}
call_function  linear_1                                                                                                                <built-in function linear>                                                                                             (embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_)                               {}
call_method    x_1                                                                                                                     view                                                                                                                   (linear_1, (1, 128, 12, 64))                                                                                                                                                                                                                                           {}
call_method    key_layer                                                                                                               permute                                                                                                                (x_1, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  linear_2                                                                                                                <built-in function linear>                                                                                             (embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_)                           {}
call_method    x_2                                                                                                                     view                                                                                                                   (linear_2, (1, 128, 12, 64))                                                                                                                                                                                                                                           {}
call_method    value_layer                                                                                                             permute                                                                                                                (x_2, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  attn_output                                                                                                             <built-in function scaled_dot_product_attention>                                                                       (query_layer, key_layer, value_layer)                                                                                                                                                                                                                                  {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_1                                                                                                           transpose                                                                                                              (attn_output, 1, 2)                                                                                                                                                                                                                                                    {}
call_method    attn_output_2                                                                                                           reshape                                                                                                                (attn_output_1, 1, 128, 768)                                                                                                                                                                                                                                           {}
call_function  hidden_states                                                                                                           <built-in function linear>                                                                                             (attn_output_2, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_)                      {}
call_function  hidden_states_1                                                                                                         <function dropout at 0x1139ae0c0>                                                                                      (hidden_states, 0.1, False, False)                                                                                                                                                                                                                                     {}
call_function  add_1                                                                                                                   <built-in function add>                                                                                                (hidden_states_1, embeddings_3)                                                                                                                                                                                                                                        {}
call_function  hidden_states_2                                                                                                         <function layer_norm at 0x1139af740>                                                                                   (add_1, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}
call_function  hidden_states_3                                                                                                         <built-in function linear>                                                                                             (hidden_states_2, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_)                                            {}
call_function  hidden_states_4                                                                                                         <built-in function gelu>                                                                                               (hidden_states_3,)                                                                                                                                                                                                                                                     {}
call_function  hidden_states_5                                                                                                         <built-in function linear>                                                                                             (hidden_states_4, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_)                                                        {}
call_function  hidden_states_6                                                                                                         <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_5, 0.1, False, False)                                                                                                                                                                                                                                   {}
call_function  add_2                                                                                                                   <built-in function add>                                                                                                (hidden_states_6, hidden_states_2)                                                                                                                                                                                                                                     {}
call_function  hidden_states_7                                                                                                         <function layer_norm at 0x1139af740>                                                                                   (add_2, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}
call_function  linear_6                                                                                                                <built-in function linear>                                                                                             (hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_)                        {}
call_method    x_3                                                                                                                     view                                                                                                                   (linear_6, (1, 128, 12, 64))                                                                                                                                                                                                                                           {}
call_method    query_layer_1                                                                                                           permute                                                                                                                (x_3, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  linear_7                                                                                                                <built-in function linear>                                                                                             (hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_)                            {}
call_method    x_4                                                                                                                     view                                                                                                                   (linear_7, (1, 128, 12, 64))                                                                                                                                                                                                                                           {}
call_method    key_layer_1                                                                                                             permute                                                                                                                (x_4, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  linear_8                                                                                                                <built-in function linear>                                                                                             (hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_)                        {}
call_method    x_5                                                                                                                     view                                                                                                                   (linear_8, (1, 128, 12, 64))                                                                                                                                                                                                                                           {}
call_method    value_layer_1                                                                                                           permute                                                                                                                (x_5, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  attn_output_3                                                                                                           <built-in function scaled_dot_product_attention>                                                                       (query_layer_1, key_layer_1, value_layer_1)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_4                                                                                                           transpose                                                                                                              (attn_output_3, 1, 2)                                                                                                                                                                                                                                                  {}
call_method    attn_output_5                                                                                                           reshape                                                                                                                (attn_output_4, 1, 128, 768)                                                                                                                                                                                                                                           {}
call_function  hidden_states_8                                                                                                         <built-in function linear>                                                                                             (attn_output_5, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_)                      {}
call_function  hidden_states_9                                                                                                         <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_8, 0.1, False, False)                                                                                                                                                                                                                                   {}
call_function  add_3                                                                                                                   <built-in function add>                                                                                                (hidden_states_9, hidden_states_7)                                                                                                                                                                                                                                     {}
call_function  hidden_states_10                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_3, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}
call_function  hidden_states_11                                                                                                        <built-in function linear>                                                                                             (hidden_states_10, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_12                                                                                                        <built-in function gelu>                                                                                               (hidden_states_11,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_13                                                                                                        <built-in function linear>                                                                                             (hidden_states_12, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_14                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_13, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_4                                                                                                                   <built-in function add>                                                                                                (hidden_states_14, hidden_states_10)                                                                                                                                                                                                                                   {}
call_function  hidden_states_15                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_4, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}
call_function  linear_12                                                                                                               <built-in function linear>                                                                                             (hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_6                                                                                                                     view                                                                                                                   (linear_12, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_2                                                                                                           permute                                                                                                                (x_6, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  linear_13                                                                                                               <built-in function linear>                                                                                             (hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_7                                                                                                                     view                                                                                                                   (linear_13, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_2                                                                                                             permute                                                                                                                (x_7, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  linear_14                                                                                                               <built-in function linear>                                                                                             (hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_8                                                                                                                     view                                                                                                                   (linear_14, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_2                                                                                                           permute                                                                                                                (x_8, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  attn_output_6                                                                                                           <built-in function scaled_dot_product_attention>                                                                       (query_layer_2, key_layer_2, value_layer_2)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_7                                                                                                           transpose                                                                                                              (attn_output_6, 1, 2)                                                                                                                                                                                                                                                  {}
call_method    attn_output_8                                                                                                           reshape                                                                                                                (attn_output_7, 1, 128, 768)                                                                                                                                                                                                                                           {}
call_function  hidden_states_16                                                                                                        <built-in function linear>                                                                                             (attn_output_8, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_)                      {}
call_function  hidden_states_17                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_16, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_5                                                                                                                   <built-in function add>                                                                                                (hidden_states_17, hidden_states_15)                                                                                                                                                                                                                                   {}
call_function  hidden_states_18                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_5, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}
call_function  hidden_states_19                                                                                                        <built-in function linear>                                                                                             (hidden_states_18, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_20                                                                                                        <built-in function gelu>                                                                                               (hidden_states_19,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_21                                                                                                        <built-in function linear>                                                                                             (hidden_states_20, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_22                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_21, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_6                                                                                                                   <built-in function add>                                                                                                (hidden_states_22, hidden_states_18)                                                                                                                                                                                                                                   {}
call_function  hidden_states_23                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_6, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}
call_function  linear_18                                                                                                               <built-in function linear>                                                                                             (hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_9                                                                                                                     view                                                                                                                   (linear_18, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_3                                                                                                           permute                                                                                                                (x_9, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}
call_function  linear_19                                                                                                               <built-in function linear>                                                                                             (hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_10                                                                                                                    view                                                                                                                   (linear_19, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_3                                                                                                             permute                                                                                                                (x_10, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_20                                                                                                               <built-in function linear>                                                                                             (hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_11                                                                                                                    view                                                                                                                   (linear_20, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_3                                                                                                           permute                                                                                                                (x_11, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_9                                                                                                           <built-in function scaled_dot_product_attention>                                                                       (query_layer_3, key_layer_3, value_layer_3)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_10                                                                                                          transpose                                                                                                              (attn_output_9, 1, 2)                                                                                                                                                                                                                                                  {}
call_method    attn_output_11                                                                                                          reshape                                                                                                                (attn_output_10, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_24                                                                                                        <built-in function linear>                                                                                             (attn_output_11, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}
call_function  hidden_states_25                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_24, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_7                                                                                                                   <built-in function add>                                                                                                (hidden_states_25, hidden_states_23)                                                                                                                                                                                                                                   {}
call_function  hidden_states_26                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_7, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}
call_function  hidden_states_27                                                                                                        <built-in function linear>                                                                                             (hidden_states_26, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_28                                                                                                        <built-in function gelu>                                                                                               (hidden_states_27,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_29                                                                                                        <built-in function linear>                                                                                             (hidden_states_28, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_30                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_29, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_8                                                                                                                   <built-in function add>                                                                                                (hidden_states_30, hidden_states_26)                                                                                                                                                                                                                                   {}
call_function  hidden_states_31                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_8, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}
call_function  linear_24                                                                                                               <built-in function linear>                                                                                             (hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_12                                                                                                                    view                                                                                                                   (linear_24, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_4                                                                                                           permute                                                                                                                (x_12, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_25                                                                                                               <built-in function linear>                                                                                             (hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_13                                                                                                                    view                                                                                                                   (linear_25, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_4                                                                                                             permute                                                                                                                (x_13, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_26                                                                                                               <built-in function linear>                                                                                             (hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_14                                                                                                                    view                                                                                                                   (linear_26, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_4                                                                                                           permute                                                                                                                (x_14, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_12                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_4, key_layer_4, value_layer_4)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_13                                                                                                          transpose                                                                                                              (attn_output_12, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_14                                                                                                          reshape                                                                                                                (attn_output_13, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_32                                                                                                        <built-in function linear>                                                                                             (attn_output_14, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}
call_function  hidden_states_33                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_32, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_9                                                                                                                   <built-in function add>                                                                                                (hidden_states_33, hidden_states_31)                                                                                                                                                                                                                                   {}
call_function  hidden_states_34                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_9, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}
call_function  hidden_states_35                                                                                                        <built-in function linear>                                                                                             (hidden_states_34, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_36                                                                                                        <built-in function gelu>                                                                                               (hidden_states_35,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_37                                                                                                        <built-in function linear>                                                                                             (hidden_states_36, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_38                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_37, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_10                                                                                                                  <built-in function add>                                                                                                (hidden_states_38, hidden_states_34)                                                                                                                                                                                                                                   {}
call_function  hidden_states_39                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_10, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}
call_function  linear_30                                                                                                               <built-in function linear>                                                                                             (hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_15                                                                                                                    view                                                                                                                   (linear_30, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_5                                                                                                           permute                                                                                                                (x_15, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_31                                                                                                               <built-in function linear>                                                                                             (hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_16                                                                                                                    view                                                                                                                   (linear_31, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_5                                                                                                             permute                                                                                                                (x_16, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_32                                                                                                               <built-in function linear>                                                                                             (hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_17                                                                                                                    view                                                                                                                   (linear_32, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_5                                                                                                           permute                                                                                                                (x_17, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_15                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_5, key_layer_5, value_layer_5)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_16                                                                                                          transpose                                                                                                              (attn_output_15, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_17                                                                                                          reshape                                                                                                                (attn_output_16, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_40                                                                                                        <built-in function linear>                                                                                             (attn_output_17, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}
call_function  hidden_states_41                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_40, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_11                                                                                                                  <built-in function add>                                                                                                (hidden_states_41, hidden_states_39)                                                                                                                                                                                                                                   {}
call_function  hidden_states_42                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_11, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}
call_function  hidden_states_43                                                                                                        <built-in function linear>                                                                                             (hidden_states_42, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_44                                                                                                        <built-in function gelu>                                                                                               (hidden_states_43,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_45                                                                                                        <built-in function linear>                                                                                             (hidden_states_44, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_46                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_45, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_12                                                                                                                  <built-in function add>                                                                                                (hidden_states_46, hidden_states_42)                                                                                                                                                                                                                                   {}
call_function  hidden_states_47                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_12, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}
call_function  linear_36                                                                                                               <built-in function linear>                                                                                             (hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_18                                                                                                                    view                                                                                                                   (linear_36, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_6                                                                                                           permute                                                                                                                (x_18, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_37                                                                                                               <built-in function linear>                                                                                             (hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_19                                                                                                                    view                                                                                                                   (linear_37, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_6                                                                                                             permute                                                                                                                (x_19, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_38                                                                                                               <built-in function linear>                                                                                             (hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_20                                                                                                                    view                                                                                                                   (linear_38, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_6                                                                                                           permute                                                                                                                (x_20, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_18                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_6, key_layer_6, value_layer_6)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_19                                                                                                          transpose                                                                                                              (attn_output_18, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_20                                                                                                          reshape                                                                                                                (attn_output_19, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_48                                                                                                        <built-in function linear>                                                                                             (attn_output_20, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}
call_function  hidden_states_49                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_48, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_13                                                                                                                  <built-in function add>                                                                                                (hidden_states_49, hidden_states_47)                                                                                                                                                                                                                                   {}
call_function  hidden_states_50                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_13, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}
call_function  hidden_states_51                                                                                                        <built-in function linear>                                                                                             (hidden_states_50, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_52                                                                                                        <built-in function gelu>                                                                                               (hidden_states_51,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_53                                                                                                        <built-in function linear>                                                                                             (hidden_states_52, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_54                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_53, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_14                                                                                                                  <built-in function add>                                                                                                (hidden_states_54, hidden_states_50)                                                                                                                                                                                                                                   {}
call_function  hidden_states_55                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_14, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}
call_function  linear_42                                                                                                               <built-in function linear>                                                                                             (hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_21                                                                                                                    view                                                                                                                   (linear_42, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_7                                                                                                           permute                                                                                                                (x_21, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_43                                                                                                               <built-in function linear>                                                                                             (hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_22                                                                                                                    view                                                                                                                   (linear_43, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_7                                                                                                             permute                                                                                                                (x_22, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_44                                                                                                               <built-in function linear>                                                                                             (hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_23                                                                                                                    view                                                                                                                   (linear_44, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_7                                                                                                           permute                                                                                                                (x_23, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_21                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_7, key_layer_7, value_layer_7)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_22                                                                                                          transpose                                                                                                              (attn_output_21, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_23                                                                                                          reshape                                                                                                                (attn_output_22, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_56                                                                                                        <built-in function linear>                                                                                             (attn_output_23, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}
call_function  hidden_states_57                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_56, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_15                                                                                                                  <built-in function add>                                                                                                (hidden_states_57, hidden_states_55)                                                                                                                                                                                                                                   {}
call_function  hidden_states_58                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_15, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}
call_function  hidden_states_59                                                                                                        <built-in function linear>                                                                                             (hidden_states_58, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_60                                                                                                        <built-in function gelu>                                                                                               (hidden_states_59,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_61                                                                                                        <built-in function linear>                                                                                             (hidden_states_60, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_62                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_61, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_16                                                                                                                  <built-in function add>                                                                                                (hidden_states_62, hidden_states_58)                                                                                                                                                                                                                                   {}
call_function  hidden_states_63                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_16, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}
call_function  linear_48                                                                                                               <built-in function linear>                                                                                             (hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_24                                                                                                                    view                                                                                                                   (linear_48, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_8                                                                                                           permute                                                                                                                (x_24, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_49                                                                                                               <built-in function linear>                                                                                             (hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_25                                                                                                                    view                                                                                                                   (linear_49, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_8                                                                                                             permute                                                                                                                (x_25, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_50                                                                                                               <built-in function linear>                                                                                             (hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_26                                                                                                                    view                                                                                                                   (linear_50, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_8                                                                                                           permute                                                                                                                (x_26, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_24                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_8, key_layer_8, value_layer_8)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_25                                                                                                          transpose                                                                                                              (attn_output_24, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_26                                                                                                          reshape                                                                                                                (attn_output_25, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_64                                                                                                        <built-in function linear>                                                                                             (attn_output_26, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}
call_function  hidden_states_65                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_64, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_17                                                                                                                  <built-in function add>                                                                                                (hidden_states_65, hidden_states_63)                                                                                                                                                                                                                                   {}
call_function  hidden_states_66                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_17, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}
call_function  hidden_states_67                                                                                                        <built-in function linear>                                                                                             (hidden_states_66, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_68                                                                                                        <built-in function gelu>                                                                                               (hidden_states_67,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_69                                                                                                        <built-in function linear>                                                                                             (hidden_states_68, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_70                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_69, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_18                                                                                                                  <built-in function add>                                                                                                (hidden_states_70, hidden_states_66)                                                                                                                                                                                                                                   {}
call_function  hidden_states_71                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_18, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}
call_function  linear_54                                                                                                               <built-in function linear>                                                                                             (hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_)                       {}
call_method    x_27                                                                                                                    view                                                                                                                   (linear_54, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_9                                                                                                           permute                                                                                                                (x_27, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_55                                                                                                               <built-in function linear>                                                                                             (hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_)                           {}
call_method    x_28                                                                                                                    view                                                                                                                   (linear_55, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_9                                                                                                             permute                                                                                                                (x_28, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_56                                                                                                               <built-in function linear>                                                                                             (hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_)                       {}
call_method    x_29                                                                                                                    view                                                                                                                   (linear_56, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_9                                                                                                           permute                                                                                                                (x_29, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_27                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_9, key_layer_9, value_layer_9)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_28                                                                                                          transpose                                                                                                              (attn_output_27, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_29                                                                                                          reshape                                                                                                                (attn_output_28, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_72                                                                                                        <built-in function linear>                                                                                             (attn_output_29, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}
call_function  hidden_states_73                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_72, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_19                                                                                                                  <built-in function add>                                                                                                (hidden_states_73, hidden_states_71)                                                                                                                                                                                                                                   {}
call_function  hidden_states_74                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_19, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}
call_function  hidden_states_75                                                                                                        <built-in function linear>                                                                                             (hidden_states_74, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_)                                           {}
call_function  hidden_states_76                                                                                                        <built-in function gelu>                                                                                               (hidden_states_75,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_77                                                                                                        <built-in function linear>                                                                                             (hidden_states_76, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_)                                                       {}
call_function  hidden_states_78                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_77, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_20                                                                                                                  <built-in function add>                                                                                                (hidden_states_78, hidden_states_74)                                                                                                                                                                                                                                   {}
call_function  hidden_states_79                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_20, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}
call_function  linear_60                                                                                                               <built-in function linear>                                                                                             (hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_)                     {}
call_method    x_30                                                                                                                    view                                                                                                                   (linear_60, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_10                                                                                                          permute                                                                                                                (x_30, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_61                                                                                                               <built-in function linear>                                                                                             (hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_)                         {}
call_method    x_31                                                                                                                    view                                                                                                                   (linear_61, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_10                                                                                                            permute                                                                                                                (x_31, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_62                                                                                                               <built-in function linear>                                                                                             (hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_)                     {}
call_method    x_32                                                                                                                    view                                                                                                                   (linear_62, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_10                                                                                                          permute                                                                                                                (x_32, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_30                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_10, key_layer_10, value_layer_10)                                                                                                                                                                                                                         {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_31                                                                                                          transpose                                                                                                              (attn_output_30, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_32                                                                                                          reshape                                                                                                                (attn_output_31, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_80                                                                                                        <built-in function linear>                                                                                             (attn_output_32, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_)                   {}
call_function  hidden_states_81                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_80, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_21                                                                                                                  <built-in function add>                                                                                                (hidden_states_81, hidden_states_79)                                                                                                                                                                                                                                   {}
call_function  hidden_states_82                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_21, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)  {}
call_function  hidden_states_83                                                                                                        <built-in function linear>                                                                                             (hidden_states_82, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_)                                         {}
call_function  hidden_states_84                                                                                                        <built-in function gelu>                                                                                               (hidden_states_83,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_85                                                                                                        <built-in function linear>                                                                                             (hidden_states_84, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_)                                                     {}
call_function  hidden_states_86                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_85, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_22                                                                                                                  <built-in function add>                                                                                                (hidden_states_86, hidden_states_82)                                                                                                                                                                                                                                   {}
call_function  hidden_states_87                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_22, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                      {}
call_function  linear_66                                                                                                               <built-in function linear>                                                                                             (hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_)                     {}
call_method    x_33                                                                                                                    view                                                                                                                   (linear_66, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    query_layer_11                                                                                                          permute                                                                                                                (x_33, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_67                                                                                                               <built-in function linear>                                                                                             (hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_)                         {}
call_method    x_34                                                                                                                    view                                                                                                                   (linear_67, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    key_layer_11                                                                                                            permute                                                                                                                (x_34, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  linear_68                                                                                                               <built-in function linear>                                                                                             (hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_)                     {}
call_method    x_35                                                                                                                    view                                                                                                                   (linear_68, (1, 128, 12, 64))                                                                                                                                                                                                                                          {}
call_method    value_layer_11                                                                                                          permute                                                                                                                (x_35, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}
call_function  attn_output_33                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_11, key_layer_11, value_layer_11)                                                                                                                                                                                                                         {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}
call_method    attn_output_34                                                                                                          transpose                                                                                                              (attn_output_33, 1, 2)                                                                                                                                                                                                                                                 {}
call_method    attn_output_35                                                                                                          reshape                                                                                                                (attn_output_34, 1, 128, 768)                                                                                                                                                                                                                                          {}
call_function  hidden_states_88                                                                                                        <built-in function linear>                                                                                             (attn_output_35, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_)                   {}
call_function  hidden_states_89                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_88, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_23                                                                                                                  <built-in function add>                                                                                                (hidden_states_89, hidden_states_87)                                                                                                                                                                                                                                   {}
call_function  hidden_states_90                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_23, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)  {}
call_function  hidden_states_91                                                                                                        <built-in function linear>                                                                                             (hidden_states_90, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_)                                         {}
call_function  hidden_states_92                                                                                                        <built-in function gelu>                                                                                               (hidden_states_91,)                                                                                                                                                                                                                                                    {}
call_function  hidden_states_93                                                                                                        <built-in function linear>                                                                                             (hidden_states_92, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_)                                                     {}
call_function  hidden_states_94                                                                                                        <function dropout at 0x1139ae0c0>                                                                                      (hidden_states_93, 0.1, False, False)                                                                                                                                                                                                                                  {}
call_function  add_24                                                                                                                  <built-in function add>                                                                                                (hidden_states_94, hidden_states_90)                                                                                                                                                                                                                                   {}
call_function  hidden_states_95                                                                                                        <function layer_norm at 0x1139af740>                                                                                   (add_24, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                      {}
call_function  first_token_tensor                                                                                                      <built-in function getitem>                                                                                            (hidden_states_95, (slice(None, None, None), 0))                                                                                                                                                                                                                       {}
call_function  pooled_output                                                                                                           <built-in function linear>                                                                                             (first_token_tensor, l_self_modules_pooler_modules_dense_parameters_weight_, l_self_modules_pooler_modules_dense_parameters_bias_)                                                                                                                                     {}
call_function  pooled_output_1                                                                                                         <built-in method tanh of type object at 0x111620088>                                                                   (pooled_output,)                                                                                                                                                                                                                                                       {}
output         output                                                                                                                  output                                                                                                                 ((hidden_states_95, pooled_output_1),)                                                                                                                                                                                                                                 {}
NODE buffered_token_type_ids
NODE position_ids
NODE inputs_embeds
NODE token_type_embeddings
NODE embeddings
NODE position_embeddings
NODE embeddings_1
NODE embeddings_2
NODE embeddings_3
NODE getitem_2
NODE inverted_mask
NODE linear
NODE linear_1
NODE linear_2
NODE attn_output
arg query_layer {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '4960706128': ("L['self']._modules['encoder']._modules['layer']._modules['0']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '4961296576': ("L['self']._modules['encoder']._modules['layer']._modules['0'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '4985757312': ("L['self']._modules['encoder']._modules['layer']._modules['0'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '4960706128': ("L['self']._modules['encoder']._modules['layer']._modules['0']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '4961296576': ("L['self']._modules['encoder']._modules['layer']._modules['0'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '4985757312': ("L['self']._modules['encoder']._modules['layer']._modules['0'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_1', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '4960706128': ("L['self']._modules['encoder']._modules['layer']._modules['0']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '4961296576': ("L['self']._modules['encoder']._modules['layer']._modules['0'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '4985757312': ("L['self']._modules['encoder']._modules['layer']._modules['0'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_2', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states
NODE hidden_states_1
NODE add_1
NODE hidden_states_2
NODE hidden_states_3
NODE hidden_states_4
NODE hidden_states_5
NODE hidden_states_6
NODE add_2
NODE hidden_states_7
NODE linear_6
NODE linear_7
NODE linear_8
NODE attn_output_3
arg query_layer_1 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '4962844400': ("L['self']._modules['encoder']._modules['layer']._modules['1']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '4962830432': ("L['self']._modules['encoder']._modules['layer']._modules['1'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '4962830384': ("L['self']._modules['encoder']._modules['layer']._modules['1'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_3', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_1 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '4962844400': ("L['self']._modules['encoder']._modules['layer']._modules['1']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '4962830432': ("L['self']._modules['encoder']._modules['layer']._modules['1'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '4962830384': ("L['self']._modules['encoder']._modules['layer']._modules['1'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_4', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_1 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '4962844400': ("L['self']._modules['encoder']._modules['layer']._modules['1']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '4962830432': ("L['self']._modules['encoder']._modules['layer']._modules['1'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '4962830384': ("L['self']._modules['encoder']._modules['layer']._modules['1'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_5', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_8
NODE hidden_states_9
NODE add_3
NODE hidden_states_10
NODE hidden_states_11
NODE hidden_states_12
NODE hidden_states_13
NODE hidden_states_14
NODE add_4
NODE hidden_states_15
NODE linear_12
NODE linear_13
NODE linear_14
NODE attn_output_6
arg query_layer_2 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000922800': ("L['self']._modules['encoder']._modules['layer']._modules['2']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000923856': ("L['self']._modules['encoder']._modules['layer']._modules['2'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000923232': ("L['self']._modules['encoder']._modules['layer']._modules['2'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_6', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_2 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000922800': ("L['self']._modules['encoder']._modules['layer']._modules['2']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000923856': ("L['self']._modules['encoder']._modules['layer']._modules['2'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000923232': ("L['self']._modules['encoder']._modules['layer']._modules['2'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_7', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_2 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000922800': ("L['self']._modules['encoder']._modules['layer']._modules['2']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000923856': ("L['self']._modules['encoder']._modules['layer']._modules['2'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000923232': ("L['self']._modules['encoder']._modules['layer']._modules['2'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_8', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_16
NODE hidden_states_17
NODE add_5
NODE hidden_states_18
NODE hidden_states_19
NODE hidden_states_20
NODE hidden_states_21
NODE hidden_states_22
NODE add_6
NODE hidden_states_23
NODE linear_18
NODE linear_19
NODE linear_20
NODE attn_output_9
arg query_layer_3 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000924624': ("L['self']._modules['encoder']._modules['layer']._modules['3']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000924672': ("L['self']._modules['encoder']._modules['layer']._modules['3'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000924720': ("L['self']._modules['encoder']._modules['layer']._modules['3'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_9', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_3 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000924624': ("L['self']._modules['encoder']._modules['layer']._modules['3']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000924672': ("L['self']._modules['encoder']._modules['layer']._modules['3'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000924720': ("L['self']._modules['encoder']._modules['layer']._modules['3'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_10', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_3 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000924624': ("L['self']._modules['encoder']._modules['layer']._modules['3']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000924672': ("L['self']._modules['encoder']._modules['layer']._modules['3'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000924720': ("L['self']._modules['encoder']._modules['layer']._modules['3'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_11', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_24
NODE hidden_states_25
NODE add_7
NODE hidden_states_26
NODE hidden_states_27
NODE hidden_states_28
NODE hidden_states_29
NODE hidden_states_30
NODE add_8
NODE hidden_states_31
NODE linear_24
NODE linear_25
NODE linear_26
NODE attn_output_12
arg query_layer_4 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000925584': ("L['self']._modules['encoder']._modules['layer']._modules['4']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000925632': ("L['self']._modules['encoder']._modules['layer']._modules['4'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000925680': ("L['self']._modules['encoder']._modules['layer']._modules['4'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_12', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_4 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000925584': ("L['self']._modules['encoder']._modules['layer']._modules['4']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000925632': ("L['self']._modules['encoder']._modules['layer']._modules['4'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000925680': ("L['self']._modules['encoder']._modules['layer']._modules['4'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_13', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_4 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000925584': ("L['self']._modules['encoder']._modules['layer']._modules['4']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000925632': ("L['self']._modules['encoder']._modules['layer']._modules['4'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000925680': ("L['self']._modules['encoder']._modules['layer']._modules['4'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_14', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_32
NODE hidden_states_33
NODE add_9
NODE hidden_states_34
NODE hidden_states_35
NODE hidden_states_36
NODE hidden_states_37
NODE hidden_states_38
NODE add_10
NODE hidden_states_39
NODE linear_30
NODE linear_31
NODE linear_32
NODE attn_output_15
arg query_layer_5 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000926544': ("L['self']._modules['encoder']._modules['layer']._modules['5']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000926592': ("L['self']._modules['encoder']._modules['layer']._modules['5'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000926640': ("L['self']._modules['encoder']._modules['layer']._modules['5'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_15', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_5 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000926544': ("L['self']._modules['encoder']._modules['layer']._modules['5']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000926592': ("L['self']._modules['encoder']._modules['layer']._modules['5'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000926640': ("L['self']._modules['encoder']._modules['layer']._modules['5'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_16', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_5 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000926544': ("L['self']._modules['encoder']._modules['layer']._modules['5']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000926592': ("L['self']._modules['encoder']._modules['layer']._modules['5'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000926640': ("L['self']._modules['encoder']._modules['layer']._modules['5'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_17', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_40
NODE hidden_states_41
NODE add_11
NODE hidden_states_42
NODE hidden_states_43
NODE hidden_states_44
NODE hidden_states_45
NODE hidden_states_46
NODE add_12
NODE hidden_states_47
NODE linear_36
NODE linear_37
NODE linear_38
NODE attn_output_18
arg query_layer_6 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000927504': ("L['self']._modules['encoder']._modules['layer']._modules['6']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000927552': ("L['self']._modules['encoder']._modules['layer']._modules['6'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000927600': ("L['self']._modules['encoder']._modules['layer']._modules['6'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_18', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_6 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000927504': ("L['self']._modules['encoder']._modules['layer']._modules['6']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000927552': ("L['self']._modules['encoder']._modules['layer']._modules['6'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000927600': ("L['self']._modules['encoder']._modules['layer']._modules['6'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_19', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_6 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000927504': ("L['self']._modules['encoder']._modules['layer']._modules['6']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000927552': ("L['self']._modules['encoder']._modules['layer']._modules['6'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000927600': ("L['self']._modules['encoder']._modules['layer']._modules['6'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_20', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_48
NODE hidden_states_49
NODE add_13
NODE hidden_states_50
NODE hidden_states_51
NODE hidden_states_52
NODE hidden_states_53
NODE hidden_states_54
NODE add_14
NODE hidden_states_55
NODE linear_42
NODE linear_43
NODE linear_44
NODE attn_output_21
arg query_layer_7 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000928464': ("L['self']._modules['encoder']._modules['layer']._modules['7']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000928512': ("L['self']._modules['encoder']._modules['layer']._modules['7'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000928560': ("L['self']._modules['encoder']._modules['layer']._modules['7'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_21', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_7 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000928464': ("L['self']._modules['encoder']._modules['layer']._modules['7']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000928512': ("L['self']._modules['encoder']._modules['layer']._modules['7'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000928560': ("L['self']._modules['encoder']._modules['layer']._modules['7'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_22', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_7 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000928464': ("L['self']._modules['encoder']._modules['layer']._modules['7']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000928512': ("L['self']._modules['encoder']._modules['layer']._modules['7'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000928560': ("L['self']._modules['encoder']._modules['layer']._modules['7'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_23', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_56
NODE hidden_states_57
NODE add_15
NODE hidden_states_58
NODE hidden_states_59
NODE hidden_states_60
NODE hidden_states_61
NODE hidden_states_62
NODE add_16
NODE hidden_states_63
NODE linear_48
NODE linear_49
NODE linear_50
NODE attn_output_24
arg query_layer_8 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000929424': ("L['self']._modules['encoder']._modules['layer']._modules['8']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000929472': ("L['self']._modules['encoder']._modules['layer']._modules['8'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000929520': ("L['self']._modules['encoder']._modules['layer']._modules['8'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_24', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_8 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000929424': ("L['self']._modules['encoder']._modules['layer']._modules['8']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000929472': ("L['self']._modules['encoder']._modules['layer']._modules['8'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000929520': ("L['self']._modules['encoder']._modules['layer']._modules['8'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_25', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_8 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000929424': ("L['self']._modules['encoder']._modules['layer']._modules['8']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000929472': ("L['self']._modules['encoder']._modules['layer']._modules['8'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000929520': ("L['self']._modules['encoder']._modules['layer']._modules['8'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_26', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_64
NODE hidden_states_65
NODE add_17
NODE hidden_states_66
NODE hidden_states_67
NODE hidden_states_68
NODE hidden_states_69
NODE hidden_states_70
NODE add_18
NODE hidden_states_71
NODE linear_54
NODE linear_55
NODE linear_56
NODE attn_output_27
arg query_layer_9 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000930384': ("L['self']._modules['encoder']._modules['layer']._modules['9']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000930432': ("L['self']._modules['encoder']._modules['layer']._modules['9'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000930480': ("L['self']._modules['encoder']._modules['layer']._modules['9'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_27', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_9 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000930384': ("L['self']._modules['encoder']._modules['layer']._modules['9']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000930432': ("L['self']._modules['encoder']._modules['layer']._modules['9'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000930480': ("L['self']._modules['encoder']._modules['layer']._modules['9'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_28', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_9 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000930384': ("L['self']._modules['encoder']._modules['layer']._modules['9']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000930432': ("L['self']._modules['encoder']._modules['layer']._modules['9'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000930480': ("L['self']._modules['encoder']._modules['layer']._modules['9'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_29', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_72
NODE hidden_states_73
NODE add_19
NODE hidden_states_74
NODE hidden_states_75
NODE hidden_states_76
NODE hidden_states_77
NODE hidden_states_78
NODE add_20
NODE hidden_states_79
NODE linear_60
NODE linear_61
NODE linear_62
NODE attn_output_30
arg query_layer_10 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000931344': ("L['self']._modules['encoder']._modules['layer']._modules['10']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000931392': ("L['self']._modules['encoder']._modules['layer']._modules['10'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000931440': ("L['self']._modules['encoder']._modules['layer']._modules['10'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_30', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_10 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000931344': ("L['self']._modules['encoder']._modules['layer']._modules['10']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000931392': ("L['self']._modules['encoder']._modules['layer']._modules['10'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000931440': ("L['self']._modules['encoder']._modules['layer']._modules['10'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_31', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_10 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000931344': ("L['self']._modules['encoder']._modules['layer']._modules['10']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000931392': ("L['self']._modules['encoder']._modules['layer']._modules['10'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000931440': ("L['self']._modules['encoder']._modules['layer']._modules['10'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_32', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_80
NODE hidden_states_81
NODE add_21
NODE hidden_states_82
NODE hidden_states_83
NODE hidden_states_84
NODE hidden_states_85
NODE hidden_states_86
NODE add_22
NODE hidden_states_87
NODE linear_66
NODE linear_67
NODE linear_68
NODE attn_output_33
arg query_layer_11 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000932304': ("L['self']._modules['encoder']._modules['layer']._modules['11']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000932352': ("L['self']._modules['encoder']._modules['layer']._modules['11'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000932400': ("L['self']._modules['encoder']._modules['layer']._modules['11'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_33', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 395, in forward\n    query_layer = self.transpose_for_scores(self.query(hidden_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg key_layer_11 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000932304': ("L['self']._modules['encoder']._modules['layer']._modules['11']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000932352': ("L['self']._modules['encoder']._modules['layer']._modules['11'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000932400': ("L['self']._modules['encoder']._modules['layer']._modules['11'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_34', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 408, in forward\n    key_layer = self.transpose_for_scores(self.key(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
arg value_layer_11 {'nn_module_stack': {'4985481328': ("L['self'].encoder", <class 'transformers.models.bert.modeling_bert.BertEncoder'>), '5000932304': ("L['self']._modules['encoder']._modules['layer']._modules['11']", <class 'transformers.models.bert.modeling_bert.BertLayer'>), '5000932352': ("L['self']._modules['encoder']._modules['layer']._modules['11'].attention", <class 'transformers.models.bert.modeling_bert.BertAttention'>), '5000932400': ("L['self']._modules['encoder']._modules['layer']._modules['11'].attention.self", <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>)}, 'source_fn_stack': [('permute_35', 'permute')], 'stack_trace': '  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 1142, in forward\n    encoder_outputs = self.encoder(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 695, in forward\n    layer_outputs = layer_module(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward\n    self_attention_outputs = self.attention(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 515, in forward\n    self_outputs = self.self(\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 409, in forward\n    value_layer = self.transpose_for_scores(self.value(current_states))\n  File "/Users/maggie/.pyenv/versions/3.12.5/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py", line 253, in transpose_for_scores\n    return x.permute(0, 2, 1, 3)\n', 'example_value': FakeTensor(..., size=(1, 12, 128, 64), grad_fn=<PermuteBackward0>)}
kwarg attn_mask
kwarg dropout_p
kwarg is_causal
NODE hidden_states_88
NODE hidden_states_89
NODE add_23
NODE hidden_states_90
NODE hidden_states_91
NODE hidden_states_92
NODE hidden_states_93
NODE hidden_states_94
NODE add_24
NODE hidden_states_95
NODE first_token_tensor
NODE pooled_output
NODE pooled_output_1



def forward(self, L_input_ids_ : torch.Tensor, L_self_modules_embeddings_buffers_token_type_ids_ : torch.Tensor, L_self_modules_embeddings_buffers_position_ids_ : torch.Tensor, L_self_modules_embeddings_modules_word_embeddings_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_position_embeddings_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_embeddings_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_attention_mask_ : torch.Tensor, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_pooler_modules_dense_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_pooler_modules_dense_parameters_bias_ : torch.nn.parameter.Parameter):
    l_input_ids_ = L_input_ids_
    l_self_modules_embeddings_buffers_token_type_ids_ = L_self_modules_embeddings_buffers_token_type_ids_
    l_self_modules_embeddings_buffers_position_ids_ = L_self_modules_embeddings_buffers_position_ids_
    l_self_modules_embeddings_modules_word_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_word_embeddings_parameters_weight_
    l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_
    l_self_modules_embeddings_modules_position_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_position_embeddings_parameters_weight_
    l_self_modules_embeddings_modules_layer_norm_parameters_weight_ = L_self_modules_embeddings_modules_LayerNorm_parameters_weight_
    l_self_modules_embeddings_modules_layer_norm_parameters_bias_ = L_self_modules_embeddings_modules_LayerNorm_parameters_bias_
    l_attention_mask_ = L_attention_mask_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_
    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_
    l_self_modules_pooler_modules_dense_parameters_weight_ = L_self_modules_pooler_modules_dense_parameters_weight_
    l_self_modules_pooler_modules_dense_parameters_bias_ = L_self_modules_pooler_modules_dense_parameters_bias_
    buffered_token_type_ids = l_self_modules_embeddings_buffers_token_type_ids_[(slice(None, None, None), slice(None, 128, None))];  l_self_modules_embeddings_buffers_token_type_ids_ = None
    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(1, 128);  buffered_token_type_ids = None
    position_ids = l_self_modules_embeddings_buffers_position_ids_[(slice(None, None, None), slice(0, 128, None))];  l_self_modules_embeddings_buffers_position_ids_ = None
    inputs_embeds = torch.nn.functional.embedding(l_input_ids_, l_self_modules_embeddings_modules_word_embeddings_parameters_weight_, 0, None, 2.0, False, False);  l_input_ids_ = l_self_modules_embeddings_modules_word_embeddings_parameters_weight_ = None
    token_type_embeddings = torch.nn.functional.embedding(buffered_token_type_ids_expanded, l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_, None, None, 2.0, False, False);  buffered_token_type_ids_expanded = l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ = None
    embeddings = inputs_embeds + token_type_embeddings;  inputs_embeds = token_type_embeddings = None
    position_embeddings = torch.nn.functional.embedding(position_ids, l_self_modules_embeddings_modules_position_embeddings_parameters_weight_, None, None, 2.0, False, False);  position_ids = l_self_modules_embeddings_modules_position_embeddings_parameters_weight_ = None
    embeddings += position_embeddings;  embeddings_1 = embeddings;  embeddings = position_embeddings = None
    embeddings_2 = torch.nn.functional.layer_norm(embeddings_1, (768,), l_self_modules_embeddings_modules_layer_norm_parameters_weight_, l_self_modules_embeddings_modules_layer_norm_parameters_bias_, 1e-12);  embeddings_1 = l_self_modules_embeddings_modules_layer_norm_parameters_weight_ = l_self_modules_embeddings_modules_layer_norm_parameters_bias_ = None
    embeddings_3 = torch.nn.functional.dropout(embeddings_2, 0.1, False, False);  embeddings_2 = None
    getitem_2 = l_attention_mask_[(slice(None, None, None), None, None, slice(None, None, None))];  l_attention_mask_ = None
    expand_1 = getitem_2.expand(1, 1, 128, 128);  getitem_2 = None
    expanded_mask = expand_1.to(torch.float32);  expand_1 = None
    inverted_mask = 1.0 - expanded_mask;  expanded_mask = None
    to_1 = inverted_mask.to(torch.bool)
    extended_attention_mask = inverted_mask.masked_fill(to_1, -3.4028234663852886e+38);  inverted_mask = to_1 = None
    linear = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x = linear.view((1, 128, 12, 64));  linear = None
    query_layer = x.permute(0, 2, 1, 3);  x = None
    linear_1 = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_1 = linear_1.view((1, 128, 12, 64));  linear_1 = None
    key_layer = x_1.permute(0, 2, 1, 3);  x_1 = None
    linear_2 = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_2 = linear_2.view((1, 128, 12, 64));  linear_2 = None
    value_layer = x_2.permute(0, 2, 1, 3);  x_2 = None
    attn_output = torch._C._nn.scaled_dot_product_attention(query_layer, key_layer, value_layer, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer = key_layer = value_layer = None
    attn_output_1 = attn_output.transpose(1, 2);  attn_output = None
    attn_output_2 = attn_output_1.reshape(1, 128, 768);  attn_output_1 = None
    hidden_states = torch._C._nn.linear(attn_output_2, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_2 = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_1 = torch.nn.functional.dropout(hidden_states, 0.1, False, False);  hidden_states = None
    add_1 = hidden_states_1 + embeddings_3;  hidden_states_1 = embeddings_3 = None
    hidden_states_2 = torch.nn.functional.layer_norm(add_1, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_1 = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_3 = torch._C._nn.linear(hidden_states_2, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_4 = torch._C._nn.gelu(hidden_states_3);  hidden_states_3 = None
    hidden_states_5 = torch._C._nn.linear(hidden_states_4, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_);  hidden_states_4 = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_6 = torch.nn.functional.dropout(hidden_states_5, 0.1, False, False);  hidden_states_5 = None
    add_2 = hidden_states_6 + hidden_states_2;  hidden_states_6 = hidden_states_2 = None
    hidden_states_7 = torch.nn.functional.layer_norm(add_2, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_2 = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_6 = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_3 = linear_6.view((1, 128, 12, 64));  linear_6 = None
    query_layer_1 = x_3.permute(0, 2, 1, 3);  x_3 = None
    linear_7 = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_4 = linear_7.view((1, 128, 12, 64));  linear_7 = None
    key_layer_1 = x_4.permute(0, 2, 1, 3);  x_4 = None
    linear_8 = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_5 = linear_8.view((1, 128, 12, 64));  linear_8 = None
    value_layer_1 = x_5.permute(0, 2, 1, 3);  x_5 = None
    attn_output_3 = torch._C._nn.scaled_dot_product_attention(query_layer_1, key_layer_1, value_layer_1, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_1 = key_layer_1 = value_layer_1 = None
    attn_output_4 = attn_output_3.transpose(1, 2);  attn_output_3 = None
    attn_output_5 = attn_output_4.reshape(1, 128, 768);  attn_output_4 = None
    hidden_states_8 = torch._C._nn.linear(attn_output_5, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_5 = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_9 = torch.nn.functional.dropout(hidden_states_8, 0.1, False, False);  hidden_states_8 = None
    add_3 = hidden_states_9 + hidden_states_7;  hidden_states_9 = hidden_states_7 = None
    hidden_states_10 = torch.nn.functional.layer_norm(add_3, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_3 = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_11 = torch._C._nn.linear(hidden_states_10, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_12 = torch._C._nn.gelu(hidden_states_11);  hidden_states_11 = None
    hidden_states_13 = torch._C._nn.linear(hidden_states_12, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_);  hidden_states_12 = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_14 = torch.nn.functional.dropout(hidden_states_13, 0.1, False, False);  hidden_states_13 = None
    add_4 = hidden_states_14 + hidden_states_10;  hidden_states_14 = hidden_states_10 = None
    hidden_states_15 = torch.nn.functional.layer_norm(add_4, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_4 = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_12 = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_6 = linear_12.view((1, 128, 12, 64));  linear_12 = None
    query_layer_2 = x_6.permute(0, 2, 1, 3);  x_6 = None
    linear_13 = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_7 = linear_13.view((1, 128, 12, 64));  linear_13 = None
    key_layer_2 = x_7.permute(0, 2, 1, 3);  x_7 = None
    linear_14 = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_8 = linear_14.view((1, 128, 12, 64));  linear_14 = None
    value_layer_2 = x_8.permute(0, 2, 1, 3);  x_8 = None
    attn_output_6 = torch._C._nn.scaled_dot_product_attention(query_layer_2, key_layer_2, value_layer_2, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_2 = key_layer_2 = value_layer_2 = None
    attn_output_7 = attn_output_6.transpose(1, 2);  attn_output_6 = None
    attn_output_8 = attn_output_7.reshape(1, 128, 768);  attn_output_7 = None
    hidden_states_16 = torch._C._nn.linear(attn_output_8, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_8 = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_17 = torch.nn.functional.dropout(hidden_states_16, 0.1, False, False);  hidden_states_16 = None
    add_5 = hidden_states_17 + hidden_states_15;  hidden_states_17 = hidden_states_15 = None
    hidden_states_18 = torch.nn.functional.layer_norm(add_5, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_5 = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_19 = torch._C._nn.linear(hidden_states_18, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_20 = torch._C._nn.gelu(hidden_states_19);  hidden_states_19 = None
    hidden_states_21 = torch._C._nn.linear(hidden_states_20, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_);  hidden_states_20 = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_22 = torch.nn.functional.dropout(hidden_states_21, 0.1, False, False);  hidden_states_21 = None
    add_6 = hidden_states_22 + hidden_states_18;  hidden_states_22 = hidden_states_18 = None
    hidden_states_23 = torch.nn.functional.layer_norm(add_6, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_6 = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_18 = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_9 = linear_18.view((1, 128, 12, 64));  linear_18 = None
    query_layer_3 = x_9.permute(0, 2, 1, 3);  x_9 = None
    linear_19 = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_10 = linear_19.view((1, 128, 12, 64));  linear_19 = None
    key_layer_3 = x_10.permute(0, 2, 1, 3);  x_10 = None
    linear_20 = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_11 = linear_20.view((1, 128, 12, 64));  linear_20 = None
    value_layer_3 = x_11.permute(0, 2, 1, 3);  x_11 = None
    attn_output_9 = torch._C._nn.scaled_dot_product_attention(query_layer_3, key_layer_3, value_layer_3, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_3 = key_layer_3 = value_layer_3 = None
    attn_output_10 = attn_output_9.transpose(1, 2);  attn_output_9 = None
    attn_output_11 = attn_output_10.reshape(1, 128, 768);  attn_output_10 = None
    hidden_states_24 = torch._C._nn.linear(attn_output_11, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_11 = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_25 = torch.nn.functional.dropout(hidden_states_24, 0.1, False, False);  hidden_states_24 = None
    add_7 = hidden_states_25 + hidden_states_23;  hidden_states_25 = hidden_states_23 = None
    hidden_states_26 = torch.nn.functional.layer_norm(add_7, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_7 = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_27 = torch._C._nn.linear(hidden_states_26, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_28 = torch._C._nn.gelu(hidden_states_27);  hidden_states_27 = None
    hidden_states_29 = torch._C._nn.linear(hidden_states_28, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_);  hidden_states_28 = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_30 = torch.nn.functional.dropout(hidden_states_29, 0.1, False, False);  hidden_states_29 = None
    add_8 = hidden_states_30 + hidden_states_26;  hidden_states_30 = hidden_states_26 = None
    hidden_states_31 = torch.nn.functional.layer_norm(add_8, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_8 = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_24 = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_12 = linear_24.view((1, 128, 12, 64));  linear_24 = None
    query_layer_4 = x_12.permute(0, 2, 1, 3);  x_12 = None
    linear_25 = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_13 = linear_25.view((1, 128, 12, 64));  linear_25 = None
    key_layer_4 = x_13.permute(0, 2, 1, 3);  x_13 = None
    linear_26 = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_14 = linear_26.view((1, 128, 12, 64));  linear_26 = None
    value_layer_4 = x_14.permute(0, 2, 1, 3);  x_14 = None
    attn_output_12 = torch._C._nn.scaled_dot_product_attention(query_layer_4, key_layer_4, value_layer_4, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_4 = key_layer_4 = value_layer_4 = None
    attn_output_13 = attn_output_12.transpose(1, 2);  attn_output_12 = None
    attn_output_14 = attn_output_13.reshape(1, 128, 768);  attn_output_13 = None
    hidden_states_32 = torch._C._nn.linear(attn_output_14, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_14 = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_33 = torch.nn.functional.dropout(hidden_states_32, 0.1, False, False);  hidden_states_32 = None
    add_9 = hidden_states_33 + hidden_states_31;  hidden_states_33 = hidden_states_31 = None
    hidden_states_34 = torch.nn.functional.layer_norm(add_9, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_9 = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_35 = torch._C._nn.linear(hidden_states_34, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_36 = torch._C._nn.gelu(hidden_states_35);  hidden_states_35 = None
    hidden_states_37 = torch._C._nn.linear(hidden_states_36, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_);  hidden_states_36 = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_38 = torch.nn.functional.dropout(hidden_states_37, 0.1, False, False);  hidden_states_37 = None
    add_10 = hidden_states_38 + hidden_states_34;  hidden_states_38 = hidden_states_34 = None
    hidden_states_39 = torch.nn.functional.layer_norm(add_10, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_10 = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_30 = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_15 = linear_30.view((1, 128, 12, 64));  linear_30 = None
    query_layer_5 = x_15.permute(0, 2, 1, 3);  x_15 = None
    linear_31 = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_16 = linear_31.view((1, 128, 12, 64));  linear_31 = None
    key_layer_5 = x_16.permute(0, 2, 1, 3);  x_16 = None
    linear_32 = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_17 = linear_32.view((1, 128, 12, 64));  linear_32 = None
    value_layer_5 = x_17.permute(0, 2, 1, 3);  x_17 = None
    attn_output_15 = torch._C._nn.scaled_dot_product_attention(query_layer_5, key_layer_5, value_layer_5, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_5 = key_layer_5 = value_layer_5 = None
    attn_output_16 = attn_output_15.transpose(1, 2);  attn_output_15 = None
    attn_output_17 = attn_output_16.reshape(1, 128, 768);  attn_output_16 = None
    hidden_states_40 = torch._C._nn.linear(attn_output_17, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_17 = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_41 = torch.nn.functional.dropout(hidden_states_40, 0.1, False, False);  hidden_states_40 = None
    add_11 = hidden_states_41 + hidden_states_39;  hidden_states_41 = hidden_states_39 = None
    hidden_states_42 = torch.nn.functional.layer_norm(add_11, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_11 = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_43 = torch._C._nn.linear(hidden_states_42, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_44 = torch._C._nn.gelu(hidden_states_43);  hidden_states_43 = None
    hidden_states_45 = torch._C._nn.linear(hidden_states_44, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_);  hidden_states_44 = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_46 = torch.nn.functional.dropout(hidden_states_45, 0.1, False, False);  hidden_states_45 = None
    add_12 = hidden_states_46 + hidden_states_42;  hidden_states_46 = hidden_states_42 = None
    hidden_states_47 = torch.nn.functional.layer_norm(add_12, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_12 = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_36 = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_18 = linear_36.view((1, 128, 12, 64));  linear_36 = None
    query_layer_6 = x_18.permute(0, 2, 1, 3);  x_18 = None
    linear_37 = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_19 = linear_37.view((1, 128, 12, 64));  linear_37 = None
    key_layer_6 = x_19.permute(0, 2, 1, 3);  x_19 = None
    linear_38 = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_20 = linear_38.view((1, 128, 12, 64));  linear_38 = None
    value_layer_6 = x_20.permute(0, 2, 1, 3);  x_20 = None
    attn_output_18 = torch._C._nn.scaled_dot_product_attention(query_layer_6, key_layer_6, value_layer_6, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_6 = key_layer_6 = value_layer_6 = None
    attn_output_19 = attn_output_18.transpose(1, 2);  attn_output_18 = None
    attn_output_20 = attn_output_19.reshape(1, 128, 768);  attn_output_19 = None
    hidden_states_48 = torch._C._nn.linear(attn_output_20, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_20 = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_49 = torch.nn.functional.dropout(hidden_states_48, 0.1, False, False);  hidden_states_48 = None
    add_13 = hidden_states_49 + hidden_states_47;  hidden_states_49 = hidden_states_47 = None
    hidden_states_50 = torch.nn.functional.layer_norm(add_13, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_13 = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_51 = torch._C._nn.linear(hidden_states_50, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_52 = torch._C._nn.gelu(hidden_states_51);  hidden_states_51 = None
    hidden_states_53 = torch._C._nn.linear(hidden_states_52, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_);  hidden_states_52 = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_54 = torch.nn.functional.dropout(hidden_states_53, 0.1, False, False);  hidden_states_53 = None
    add_14 = hidden_states_54 + hidden_states_50;  hidden_states_54 = hidden_states_50 = None
    hidden_states_55 = torch.nn.functional.layer_norm(add_14, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_14 = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_42 = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_21 = linear_42.view((1, 128, 12, 64));  linear_42 = None
    query_layer_7 = x_21.permute(0, 2, 1, 3);  x_21 = None
    linear_43 = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_22 = linear_43.view((1, 128, 12, 64));  linear_43 = None
    key_layer_7 = x_22.permute(0, 2, 1, 3);  x_22 = None
    linear_44 = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_23 = linear_44.view((1, 128, 12, 64));  linear_44 = None
    value_layer_7 = x_23.permute(0, 2, 1, 3);  x_23 = None
    attn_output_21 = torch._C._nn.scaled_dot_product_attention(query_layer_7, key_layer_7, value_layer_7, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_7 = key_layer_7 = value_layer_7 = None
    attn_output_22 = attn_output_21.transpose(1, 2);  attn_output_21 = None
    attn_output_23 = attn_output_22.reshape(1, 128, 768);  attn_output_22 = None
    hidden_states_56 = torch._C._nn.linear(attn_output_23, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_23 = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_57 = torch.nn.functional.dropout(hidden_states_56, 0.1, False, False);  hidden_states_56 = None
    add_15 = hidden_states_57 + hidden_states_55;  hidden_states_57 = hidden_states_55 = None
    hidden_states_58 = torch.nn.functional.layer_norm(add_15, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_15 = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_59 = torch._C._nn.linear(hidden_states_58, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_60 = torch._C._nn.gelu(hidden_states_59);  hidden_states_59 = None
    hidden_states_61 = torch._C._nn.linear(hidden_states_60, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_);  hidden_states_60 = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_62 = torch.nn.functional.dropout(hidden_states_61, 0.1, False, False);  hidden_states_61 = None
    add_16 = hidden_states_62 + hidden_states_58;  hidden_states_62 = hidden_states_58 = None
    hidden_states_63 = torch.nn.functional.layer_norm(add_16, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_16 = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_48 = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_24 = linear_48.view((1, 128, 12, 64));  linear_48 = None
    query_layer_8 = x_24.permute(0, 2, 1, 3);  x_24 = None
    linear_49 = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_25 = linear_49.view((1, 128, 12, 64));  linear_49 = None
    key_layer_8 = x_25.permute(0, 2, 1, 3);  x_25 = None
    linear_50 = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_26 = linear_50.view((1, 128, 12, 64));  linear_50 = None
    value_layer_8 = x_26.permute(0, 2, 1, 3);  x_26 = None
    attn_output_24 = torch._C._nn.scaled_dot_product_attention(query_layer_8, key_layer_8, value_layer_8, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_8 = key_layer_8 = value_layer_8 = None
    attn_output_25 = attn_output_24.transpose(1, 2);  attn_output_24 = None
    attn_output_26 = attn_output_25.reshape(1, 128, 768);  attn_output_25 = None
    hidden_states_64 = torch._C._nn.linear(attn_output_26, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_26 = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_65 = torch.nn.functional.dropout(hidden_states_64, 0.1, False, False);  hidden_states_64 = None
    add_17 = hidden_states_65 + hidden_states_63;  hidden_states_65 = hidden_states_63 = None
    hidden_states_66 = torch.nn.functional.layer_norm(add_17, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_17 = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_67 = torch._C._nn.linear(hidden_states_66, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_68 = torch._C._nn.gelu(hidden_states_67);  hidden_states_67 = None
    hidden_states_69 = torch._C._nn.linear(hidden_states_68, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_);  hidden_states_68 = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_70 = torch.nn.functional.dropout(hidden_states_69, 0.1, False, False);  hidden_states_69 = None
    add_18 = hidden_states_70 + hidden_states_66;  hidden_states_70 = hidden_states_66 = None
    hidden_states_71 = torch.nn.functional.layer_norm(add_18, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_18 = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_54 = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_27 = linear_54.view((1, 128, 12, 64));  linear_54 = None
    query_layer_9 = x_27.permute(0, 2, 1, 3);  x_27 = None
    linear_55 = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_28 = linear_55.view((1, 128, 12, 64));  linear_55 = None
    key_layer_9 = x_28.permute(0, 2, 1, 3);  x_28 = None
    linear_56 = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_29 = linear_56.view((1, 128, 12, 64));  linear_56 = None
    value_layer_9 = x_29.permute(0, 2, 1, 3);  x_29 = None
    attn_output_27 = torch._C._nn.scaled_dot_product_attention(query_layer_9, key_layer_9, value_layer_9, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_9 = key_layer_9 = value_layer_9 = None
    attn_output_28 = attn_output_27.transpose(1, 2);  attn_output_27 = None
    attn_output_29 = attn_output_28.reshape(1, 128, 768);  attn_output_28 = None
    hidden_states_72 = torch._C._nn.linear(attn_output_29, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_29 = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_73 = torch.nn.functional.dropout(hidden_states_72, 0.1, False, False);  hidden_states_72 = None
    add_19 = hidden_states_73 + hidden_states_71;  hidden_states_73 = hidden_states_71 = None
    hidden_states_74 = torch.nn.functional.layer_norm(add_19, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_19 = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_75 = torch._C._nn.linear(hidden_states_74, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_76 = torch._C._nn.gelu(hidden_states_75);  hidden_states_75 = None
    hidden_states_77 = torch._C._nn.linear(hidden_states_76, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_);  hidden_states_76 = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_78 = torch.nn.functional.dropout(hidden_states_77, 0.1, False, False);  hidden_states_77 = None
    add_20 = hidden_states_78 + hidden_states_74;  hidden_states_78 = hidden_states_74 = None
    hidden_states_79 = torch.nn.functional.layer_norm(add_20, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_20 = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_60 = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_30 = linear_60.view((1, 128, 12, 64));  linear_60 = None
    query_layer_10 = x_30.permute(0, 2, 1, 3);  x_30 = None
    linear_61 = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_31 = linear_61.view((1, 128, 12, 64));  linear_61 = None
    key_layer_10 = x_31.permute(0, 2, 1, 3);  x_31 = None
    linear_62 = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_32 = linear_62.view((1, 128, 12, 64));  linear_62 = None
    value_layer_10 = x_32.permute(0, 2, 1, 3);  x_32 = None
    attn_output_30 = torch._C._nn.scaled_dot_product_attention(query_layer_10, key_layer_10, value_layer_10, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_10 = key_layer_10 = value_layer_10 = None
    attn_output_31 = attn_output_30.transpose(1, 2);  attn_output_30 = None
    attn_output_32 = attn_output_31.reshape(1, 128, 768);  attn_output_31 = None
    hidden_states_80 = torch._C._nn.linear(attn_output_32, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_32 = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_81 = torch.nn.functional.dropout(hidden_states_80, 0.1, False, False);  hidden_states_80 = None
    add_21 = hidden_states_81 + hidden_states_79;  hidden_states_81 = hidden_states_79 = None
    hidden_states_82 = torch.nn.functional.layer_norm(add_21, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_21 = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_83 = torch._C._nn.linear(hidden_states_82, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_84 = torch._C._nn.gelu(hidden_states_83);  hidden_states_83 = None
    hidden_states_85 = torch._C._nn.linear(hidden_states_84, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_);  hidden_states_84 = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_86 = torch.nn.functional.dropout(hidden_states_85, 0.1, False, False);  hidden_states_85 = None
    add_22 = hidden_states_86 + hidden_states_82;  hidden_states_86 = hidden_states_82 = None
    hidden_states_87 = torch.nn.functional.layer_norm(add_22, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_22 = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_ = None
    linear_66 = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ = None
    x_33 = linear_66.view((1, 128, 12, 64));  linear_66 = None
    query_layer_11 = x_33.permute(0, 2, 1, 3);  x_33 = None
    linear_67 = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ = None
    x_34 = linear_67.view((1, 128, 12, 64));  linear_67 = None
    key_layer_11 = x_34.permute(0, 2, 1, 3);  x_34 = None
    linear_68 = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ = None
    x_35 = linear_68.view((1, 128, 12, 64));  linear_68 = None
    value_layer_11 = x_35.permute(0, 2, 1, 3);  x_35 = None
    attn_output_33 = torch._C._nn.scaled_dot_product_attention(query_layer_11, key_layer_11, value_layer_11, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_11 = key_layer_11 = value_layer_11 = extended_attention_mask = None
    attn_output_34 = attn_output_33.transpose(1, 2);  attn_output_33 = None
    attn_output_35 = attn_output_34.reshape(1, 128, 768);  attn_output_34 = None
    hidden_states_88 = torch._C._nn.linear(attn_output_35, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_35 = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_89 = torch.nn.functional.dropout(hidden_states_88, 0.1, False, False);  hidden_states_88 = None
    add_23 = hidden_states_89 + hidden_states_87;  hidden_states_89 = hidden_states_87 = None
    hidden_states_90 = torch.nn.functional.layer_norm(add_23, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_23 = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None
    hidden_states_91 = torch._C._nn.linear(hidden_states_90, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ = None
    hidden_states_92 = torch._C._nn.gelu(hidden_states_91);  hidden_states_91 = None
    hidden_states_93 = torch._C._nn.linear(hidden_states_92, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_);  hidden_states_92 = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ = None
    hidden_states_94 = torch.nn.functional.dropout(hidden_states_93, 0.1, False, False);  hidden_states_93 = None
    add_24 = hidden_states_94 + hidden_states_90;  hidden_states_94 = hidden_states_90 = None
    hidden_states_95 = torch.nn.functional.layer_norm(add_24, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_24 = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_ = None
    first_token_tensor = hidden_states_95[(slice(None, None, None), 0)]
    pooled_output = torch._C._nn.linear(first_token_tensor, l_self_modules_pooler_modules_dense_parameters_weight_, l_self_modules_pooler_modules_dense_parameters_bias_);  first_token_tensor = l_self_modules_pooler_modules_dense_parameters_weight_ = l_self_modules_pooler_modules_dense_parameters_bias_ = None
    pooled_output_1 = torch.tanh(pooled_output);  pooled_output = None
    return (hidden_states_95, pooled_output_1)
    
BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3321,  0.0932,  0.4117,  ..., -0.2848,  0.0262, -0.0662],
         [ 0.1247,  0.0723,  0.4196,  ..., -0.1501, -0.0843,  0.1952],
         [ 0.5054,  0.1235,  0.5434,  ..., -0.2432, -0.2263,  0.1830],
         ...,
         [ 0.3006,  0.0587,  0.4628,  ..., -0.4405,  0.0777,  0.1792],
         [ 0.2552, -0.0631,  0.4570,  ..., -0.4720,  0.0157,  0.0598],
         [ 0.2959,  0.0084,  0.5094,  ..., -0.4148,  0.0755,  0.1682]]],
       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.2808e-01, -2.0164e-01,  9.2094e-01,  4.7426e-01, -7.2971e-01,
         -4.2881e-02,  7.3082e-01,  3.1899e-01,  8.7517e-01, -9.9792e-01,
          7.4079e-01, -7.5211e-01,  9.7971e-01, -6.2706e-01,  9.5523e-01,
         -2.1660e-01,  4.5402e-01, -4.6394e-01,  4.0955e-01, -6.3912e-01,
          7.6842e-01, -7.5255e-01,  7.7896e-01,  2.0598e-01,  2.3422e-01,
         -9.0121e-01, -1.4293e-01,  9.4861e-01,  9.3308e-01,  8.0278e-01,
         -6.6629e-01,  2.2200e-01, -9.8171e-01, -2.5843e-01,  8.1263e-01,
         -9.6871e-01, -4.0845e-02, -7.0280e-01, -2.1237e-01, -6.3116e-02,
         -9.1280e-01,  2.0252e-01,  9.3704e-01, -4.2739e-01, -1.0717e-01,
         -3.6359e-01, -8.4614e-01,  2.1882e-01, -8.6547e-01, -9.5444e-01,
         -8.6299e-01, -9.3782e-01,  1.0986e-01,  1.6611e-01,  1.5993e-01,
          4.5694e-01, -7.0311e-02,  1.9874e-01, -8.3012e-02, -3.8325e-01,
         -5.8446e-01,  2.6267e-01,  6.8755e-01, -8.8175e-01, -9.5103e-01,
         -9.2167e-01, -2.7896e-01, -3.3554e-01,  1.3609e-02,  1.1551e-01,
          6.1828e-01,  1.6660e-01,  4.1357e-01, -8.6182e-01, -9.2454e-01,
          1.6874e-01, -1.8257e-01,  9.7835e-01,  1.3331e-01, -9.7778e-01,
         -7.7536e-01, -8.1370e-01,  9.7866e-02,  9.0113e-01, -8.3772e-01,
         -9.5633e-01, -1.0930e-03, -1.1926e-01, -9.8784e-01,  2.7318e-01,
          1.1029e-01,  6.2858e-02, -7.1963e-01,  7.8680e-02,  8.4296e-02,
         -9.5354e-02, -4.0922e-02,  8.8911e-01, -2.2528e-01,  4.2187e-02,
          2.2398e-01, -1.7997e-01,  1.1970e-01, -1.3492e-01,  8.6628e-02,
         -2.1065e-01, -3.7667e-01, -2.2428e-01, -4.3457e-01,  4.0447e-01,
          1.7806e-01, -2.2921e-01,  2.9946e-02, -9.1786e-01,  4.9899e-01,
         -1.9566e-01, -9.7262e-01, -1.6088e-01, -9.8652e-01,  6.4581e-01,
          2.8790e-01, -2.0835e-01,  9.5629e-01,  7.0230e-01,  1.2386e-01,
         -6.6011e-02,  9.6024e-01, -9.8344e-01,  3.4792e-01,  1.8389e-01,
          5.0460e-01, -1.3376e-01, -9.6661e-01, -9.3971e-01,  3.3188e-01,
          9.3974e-01,  6.2995e-02,  9.4298e-01, -1.2868e-01,  9.2334e-01,
          7.8476e-01,  4.8331e-01, -8.0450e-01, -4.2130e-01, -2.4731e-01,
          1.4714e-01, -6.7807e-01,  1.2721e-01,  3.6921e-01, -5.7168e-01,
          4.3211e-01, -1.9714e-01,  8.6682e-01, -9.1554e-01, -3.4519e-01,
          9.5152e-01,  7.2024e-01,  9.5241e-01,  8.7187e-01, -1.1195e-01,
         -1.4622e-01,  7.6521e-01, -3.2170e-01,  3.7941e-01,  3.2806e-01,
          2.6323e-01, -5.4610e-01,  2.2833e-01, -7.5667e-01,  5.9803e-01,
          1.9303e-01, -3.1111e-02,  8.4851e-01, -9.7245e-01, -8.9795e-02,
          3.8424e-01,  9.7410e-01,  7.0671e-01,  1.1895e-01, -8.0547e-01,
         -1.6621e-01, -4.4735e-01, -9.3099e-01,  9.6371e-01, -4.7059e-02,
          1.7112e-01,  3.3489e-01, -3.7378e-01, -8.2067e-01, -4.5818e-01,
          7.1217e-01,  6.5340e-01, -8.4127e-01, -4.1065e-02, -4.0576e-01,
         -3.4947e-01,  5.4951e-01,  4.2606e-01, -2.0330e-01, -4.5889e-01,
         -1.2492e-01,  9.3392e-01,  8.0160e-01,  6.8524e-01, -8.5403e-01,
          7.1983e-02, -9.0561e-01, -4.9173e-01,  8.0238e-02,  2.1991e-01,
         -4.7652e-02,  9.8699e-01,  3.4615e-01,  4.4601e-02, -8.3638e-01,
         -9.8046e-01,  1.4873e-01, -7.8110e-01,  2.2740e-02, -4.8666e-01,
         -1.0179e-02,  6.0505e-01, -7.9652e-01,  2.5817e-01, -9.0833e-01,
         -7.9902e-01,  2.9136e-01, -7.4125e-02,  2.1213e-01, -2.7891e-01,
          5.3652e-01, -8.2808e-01, -5.3058e-01,  5.9905e-01,  9.1185e-01,
          9.0931e-01, -7.5985e-01,  8.0380e-01, -2.1681e-01,  7.7386e-01,
         -4.6195e-01,  9.5857e-01, -8.4500e-01, -2.1673e-01, -9.1312e-01,
          7.4160e-01, -7.7651e-01,  8.0141e-01, -1.6545e-02, -8.6391e-01,
         -9.3397e-01,  1.0574e-01,  5.9966e-02,  8.7323e-01, -1.6366e-01,
          9.3978e-01, -5.4642e-01, -9.4214e-01, -2.0441e-01,  3.4139e-01,
         -9.7979e-01, -7.0384e-01,  1.2966e-01, -7.5725e-01, -2.1926e-01,
         -7.3331e-02, -9.5111e-01,  7.2708e-01,  1.8734e-01,  9.4206e-01,
         -7.4191e-02, -7.3924e-01,  2.4409e-01, -9.0516e-01, -1.9333e-02,
         -2.2772e-02,  9.3504e-01, -5.3849e-02, -9.4391e-01,  3.5304e-01,
          4.9918e-01,  2.1776e-01,  9.5165e-01,  9.4049e-01,  1.6107e-01,
          9.7136e-01,  8.7815e-01,  7.0704e-01,  6.8639e-01,  2.4563e-01,
          9.9927e-01,  7.9702e-01, -8.7274e-01, -9.1400e-01, -2.1530e-01,
          5.4076e-01, -9.8251e-01, -1.3991e-01, -7.5723e-02, -9.0229e-01,
         -8.2771e-01,  9.7135e-01,  9.3310e-01, -9.6039e-01,  7.9186e-01,
          9.2673e-01, -2.3877e-01, -9.0114e-01, -1.4686e-01,  9.5841e-01,
         -1.8057e-01,  4.7287e-01, -2.4378e-01,  2.8413e-01,  8.1332e-01,
         -7.9476e-01,  8.7867e-01,  7.9797e-01, -8.6320e-01,  2.2636e-01,
         -3.5599e-01, -9.3703e-01, -2.7493e-01, -1.3780e-01, -3.4507e-01,
         -9.5049e-01, -1.4792e-01, -8.4164e-01,  2.4455e-01,  3.6432e-02,
          1.2729e-01, -7.5254e-01, -1.2680e-02, -7.5723e-01,  4.0511e-01,
          2.2113e-01, -8.9231e-01, -5.7902e-01,  3.0124e-01, -6.6671e-01,
          7.3733e-01, -9.5024e-01,  9.6989e-01, -2.3121e-01, -9.1904e-01,
          9.5656e-01, -2.1600e-01, -8.8763e-01, -1.7763e-01,  3.5255e-02,
         -1.3135e-01,  9.6454e-01, -2.3846e-01, -9.7428e-01, -2.2596e-01,
         -2.8353e-01, -2.7894e-01, -1.2374e-01,  9.9177e-01, -8.6035e-02,
          8.8212e-01,  8.1083e-01,  9.6947e-01, -9.8344e-01, -9.1406e-01,
         -8.4207e-01, -9.5813e-01,  9.5046e-01,  9.3615e-01,  4.7585e-01,
         -4.9930e-01,  8.4784e-02,  6.4375e-01,  1.9211e-01, -8.8833e-01,
          2.2983e-01,  4.1184e-01, -2.1536e-01,  9.1026e-01, -5.9673e-01,
         -1.2275e-01,  3.6142e-01,  7.0829e-01,  7.2567e-01, -8.6687e-01,
          3.4394e-01, -2.7459e-01,  1.8311e-01, -2.6478e-01, -2.8637e-01,
         -9.6084e-01, -4.1680e-01,  9.3671e-01,  2.8660e-01, -8.5223e-01,
          4.0089e-01, -7.5243e-02, -1.5221e-01, -4.8729e-02,  1.0389e-01,
         -1.4925e-01, -7.9723e-01, -8.2326e-01, -7.4902e-01, -9.8215e-01,
          7.3169e-01,  1.2475e-01, -1.7621e-01,  7.8053e-01, -2.2607e-01,
          1.0094e-01, -5.3150e-01, -9.4619e-01,  2.2548e-01,  4.1196e-01,
         -9.5735e-01,  9.5439e-01, -1.3003e-01,  1.9923e-01,  7.5009e-01,
          9.4907e-01, -3.2868e-01, -4.7890e-01,  1.3134e-01, -9.2942e-01,
         -8.0615e-02, -9.2645e-01,  9.5060e-01, -9.6301e-01,  1.6678e-01,
          1.4004e-01, -6.7037e-01,  9.3789e-01, -1.0791e-01,  5.4914e-01,
         -1.5300e-01,  7.4174e-01,  9.0495e-01, -6.2828e-01, -3.7263e-01,
          7.4236e-02,  9.3423e-01, -8.9705e-02,  7.2100e-02, -9.5538e-01,
         -9.3285e-01, -7.1512e-01, -8.8315e-01, -9.7966e-01,  7.3439e-01,
          4.9671e-03,  1.4297e-01,  7.7254e-01, -4.1411e-01, -6.0479e-01,
         -5.3993e-01, -1.7984e-01, -9.3568e-01,  9.1777e-01, -1.6359e-01,
          2.9971e-01, -2.6909e-01,  1.9376e-01, -9.2309e-01,  7.7020e-01,
          8.6079e-01,  4.1933e-01, -2.5610e-01, -7.2588e-01,  4.2228e-01,
         -6.9120e-01,  9.1683e-01, -1.0329e-01,  9.7512e-01, -9.0128e-02,
         -8.5396e-01,  6.7514e-01,  4.3692e-01, -7.7829e-02,  2.3091e-01,
         -8.1962e-01,  1.9751e-01,  8.2773e-01,  9.5849e-01, -4.4204e-01,
         -1.4566e-01,  3.3360e-01, -7.5043e-01, -7.5655e-01,  7.5231e-01,
         -5.4481e-01,  5.3011e-02, -5.5183e-02,  2.6594e-01,  9.3565e-01,
         -2.0353e-01, -1.5090e-01, -6.0880e-01,  6.5857e-03, -2.3719e-01,
         -3.9513e-01,  8.1936e-01,  2.7021e-01, -4.3143e-01, -9.8587e-01,
          8.9901e-01, -8.2199e-01, -2.0008e-01,  8.1412e-01, -7.9631e-01,
          6.5515e-02, -2.9344e-01, -2.1846e-01,  2.6185e-01, -1.4717e-01,
         -2.2488e-01,  1.3687e-01,  2.0054e-01,  9.5812e-01, -3.3911e-01,
         -9.5801e-01, -6.4379e-01,  1.5143e-01, -9.4279e-01, -7.7472e-01,
         -3.4079e-01, -1.5994e-01, -1.3392e-02,  3.9560e-01,  7.1669e-01,
         -1.2233e-02, -9.6921e-01, -1.2507e-01, -1.5617e-01,  9.6309e-01,
          1.3436e-01, -1.4447e-01, -9.1231e-01, -9.3306e-01, -7.9872e-01,
          9.5393e-01, -9.4409e-01,  9.6551e-01, -9.6980e-01, -6.4778e-02,
          9.1072e-01,  1.3647e-01, -9.1257e-01,  1.4834e-01, -3.0114e-01,
          7.6302e-02,  5.8626e-01,  2.5618e-01, -9.5319e-01, -1.9513e-01,
         -1.2960e-01,  1.3437e-01, -8.6060e-02,  1.8457e-01,  5.9962e-01,
          1.8166e-01,  1.2081e-02, -3.9535e-01, -5.7228e-02,  2.0833e-01,
          4.4467e-01, -2.1612e-01, -7.0121e-02,  1.6190e-01, -1.0572e-01,
         -9.4389e-01, -2.3022e-01,  9.8258e-03,  4.9971e-01,  5.3343e-01,
         -9.6853e-01, -6.8418e-01, -8.5110e-01, -1.8665e-01,  8.2239e-01,
         -1.3118e-01, -6.3341e-01, -6.5414e-01,  9.4615e-01,  9.6543e-01,
          7.6989e-01, -6.1908e-02,  8.0998e-01, -7.3890e-01,  5.7653e-02,
         -6.7192e-02,  2.5213e-01,  6.3660e-01,  7.1008e-01, -2.3969e-01,
          9.8144e-01,  8.6754e-02,  5.6464e-02, -8.4386e-01,  2.6670e-01,
         -1.9047e-01,  3.1611e-01, -6.8685e-01, -9.3659e-01,  2.4767e-01,
         -2.3370e-02, -7.8819e-01,  8.1074e-02, -3.7078e-03, -2.7827e-01,
          8.8056e-01,  9.6554e-01,  6.8348e-01, -2.0116e-01,  2.8252e-01,
         -1.8369e-01, -9.0078e-02,  1.0020e-01, -9.3203e-01,  9.8459e-01,
          1.2181e-01,  7.0834e-01,  2.6737e-01,  1.6843e-01,  9.5108e-01,
          2.8272e-01,  6.6297e-01,  4.7363e-02,  8.9189e-01,  1.8120e-01,
         -9.1566e-01,  3.9009e-01, -9.6886e-01, -1.7748e-01, -9.2443e-01,
          2.3154e-01,  1.9871e-01,  8.3521e-01, -1.2835e-01,  9.5581e-01,
          9.2498e-01,  5.1863e-02,  7.1700e-01,  8.9656e-01,  2.5082e-01,
         -9.1826e-01, -9.8518e-01, -9.8551e-01, -2.6455e-01, -2.6711e-01,
         -1.4622e-01,  3.4950e-01,  2.2648e-01,  1.6545e-01,  1.5553e-01,
         -8.2077e-01,  8.9977e-01,  3.1285e-01, -9.0760e-01,  9.5463e-01,
         -2.5736e-01,  1.1502e-03,  1.4512e-01, -9.8429e-01, -8.8767e-01,
         -2.3046e-01, -1.8412e-01,  7.6753e-01,  4.5756e-01,  8.1062e-01,
          1.8514e-01, -3.9613e-01, -2.8621e-01,  7.7372e-01, -2.0723e-01,
         -9.8607e-01,  3.0619e-01,  8.6776e-01, -8.0016e-01,  9.5634e-01,
         -7.2052e-01, -2.1562e-01,  8.3660e-01,  7.4068e-01,  7.8417e-01,
          7.5691e-01,  4.8504e-01,  1.5010e-01,  7.0401e-01,  9.0009e-01,
          8.7273e-01,  9.8282e-01,  8.5639e-01,  5.4819e-01,  8.4727e-01,
          3.2496e-01,  5.0530e-01, -9.1311e-01,  1.0156e-01, -2.6760e-01,
          1.2808e-01,  1.7161e-01, -1.4114e-01, -8.7623e-01,  4.0321e-01,
         -1.4807e-01,  3.6025e-01, -2.0411e-01,  7.4031e-02, -2.9507e-01,
         -8.6289e-02, -5.9803e-01,  2.1920e-01,  2.6644e-01,  4.6553e-02,
          9.3025e-01, -3.8382e-01, -6.3695e-02, -1.2815e-01, -6.5796e-02,
          8.8562e-01, -8.7186e-01,  7.4126e-01,  4.9767e-04,  7.1476e-01,
         -8.0303e-01, -2.2803e-02,  5.8500e-01, -6.8119e-01, -1.7543e-01,
         -1.1267e-01, -6.4594e-01,  8.3012e-01,  1.2663e-01, -2.6006e-01,
         -1.5632e-01,  4.7296e-01,  2.8375e-01, -6.6267e-01,  8.2547e-01,
          9.3511e-01,  1.6140e-01, -1.1795e-01,  1.7005e-01,  6.0164e-02,
         -7.6665e-01,  3.0834e-01,  6.4293e-01, -7.4522e-01,  8.4781e-01,
         -8.6458e-01,  7.2301e-01, -9.2949e-01, -1.4941e-01, -2.0569e-01,
         -8.3325e-01, -3.5431e-01,  3.6049e-01,  1.9994e-01,  8.3253e-01,
         -9.3162e-01,  8.5115e-01,  5.8544e-01,  8.0342e-01,  2.1705e-01,
          7.8114e-01, -5.4500e-01,  8.3833e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)
